{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6178328f",
   "metadata": {},
   "source": [
    "# 🚀 PaniniFS - Mode Cloud Autonome\n",
    "\n",
    "**100% Cloud Native** - Clonage automatique des repos GitHub\n",
    "\n",
    "## 🎯 Workflow Autonome\n",
    "1. **Auto-détection** : Mode Colab vs Local\n",
    "2. **Clonage repos** : Tous les repos GitHub automatiquement\n",
    "3. **Scan optimisé** : Limites strictes pour performance\n",
    "4. **Embeddings** : Pipeline complet temps réel\n",
    "5. **Recherche** : Interface interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e609e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 SETUP AUTONOME - Détection environnement et installation\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Détection mode Cloud (Colab/Kaggle/etc)\n",
    "IS_CLOUD = 'google.colab' in sys.modules or '/kaggle/' in os.environ.get('PATH', '') or 'COLAB_GPU' in os.environ\n",
    "print(f\"🌍 Mode détecté: {'☁️ CLOUD' if IS_CLOUD else '🖥️ LOCAL'}\")\n",
    "\n",
    "if IS_CLOUD:\n",
    "    # Installation dépendances cloud\n",
    "    print(\"📦 Installation dépendances cloud...\")\n",
    "    !pip install sentence-transformers torch --quiet\n",
    "    BASE_PATH = Path('/content')\n",
    "else:\n",
    "    BASE_PATH = Path('/home/stephane/GitHub')\n",
    "\n",
    "print(f\"📁 Répertoire de travail: {BASE_PATH}\")\n",
    "os.chdir(BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 CLONAGE AUTOMATIQUE DES REPOS\n",
    "def clone_repos_cloud():\n",
    "    \"\"\"Clone tous les repos nécessaires en mode cloud\"\"\"\n",
    "    \n",
    "    repos_config = {\n",
    "        'PaniniFS-1': 'https://github.com/stephanedenis/PaniniFS.git',\n",
    "        'Pensine': 'https://github.com/stephanedenis/Pensine.git', \n",
    "        'totoro-automation': 'https://github.com/stephanedenis/totoro-automation.git',\n",
    "        'hexagonal-demo': 'https://github.com/stephanedenis/hexagonal-demo.git'\n",
    "    }\n",
    "    \n",
    "    cloned_repos = []\n",
    "    \n",
    "    for repo_name, repo_url in repos_config.items():\n",
    "        repo_path = BASE_PATH / repo_name\n",
    "        \n",
    "        if repo_path.exists():\n",
    "            print(f\"✅ {repo_name} déjà présent\")\n",
    "            cloned_repos.append(repo_name)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"📥 Clonage {repo_name}...\")\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url, str(repo_path)], \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"✅ {repo_name} cloné avec succès\")\n",
    "                cloned_repos.append(repo_name)\n",
    "            else:\n",
    "                print(f\"⚠️ Erreur clonage {repo_name}: {result.stderr}\")\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"⏱️ Timeout clonage {repo_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur {repo_name}: {e}\")\n",
    "    \n",
    "    return cloned_repos\n",
    "\n",
    "# Exécution clonage en mode cloud\n",
    "if IS_CLOUD:\n",
    "    start_time = time.time()\n",
    "    available_repos = clone_repos_cloud()\n",
    "    clone_time = time.time() - start_time\n",
    "    print(f\"\\n🎯 Clonage terminé en {clone_time:.2f}s\")\n",
    "    print(f\"📦 {len(available_repos)} repos disponibles: {', '.join(available_repos)}\")\n",
    "else:\n",
    "    # Mode local - utilise les repos existants\n",
    "    available_repos = ['PaniniFS-1', 'Pensine', 'totoro-automation', 'hexagonal-demo']\n",
    "    print(f\"📦 Mode local - {len(available_repos)} repos configurés\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 SCAN SOURCES CLOUD-OPTIMISÉ\n",
    "def scan_sources_cloud_optimized():\n",
    "    \"\"\"Scan optimisé pour mode cloud avec limites strictes\"\"\"\n",
    "    \n",
    "    # Limites cloud-optimisées\n",
    "    MAX_PY_FILES_PER_REPO = 30  # Réduit pour cloud\n",
    "    MAX_MD_FILES_PER_REPO = 15  # Réduit pour cloud\n",
    "    MAX_FILE_SIZE = 100 * 1024  # 100KB max\n",
    "    \n",
    "    all_sources = []\n",
    "    scan_stats = {'total_files': 0, 'total_size': 0, 'repos_scanned': 0}\n",
    "    \n",
    "    for repo_name in available_repos:\n",
    "        repo_path = BASE_PATH / repo_name\n",
    "        \n",
    "        if not repo_path.exists():\n",
    "            print(f\"⚠️ Repo {repo_name} non trouvé\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"🔍 Scan {repo_name}...\")\n",
    "        repo_sources = []\n",
    "        py_count, md_count = 0, 0\n",
    "        \n",
    "        try:\n",
    "            # Scan fichiers Python\n",
    "            for py_file in repo_path.rglob('*.py'):\n",
    "                if py_count >= MAX_PY_FILES_PER_REPO:\n",
    "                    break\n",
    "                    \n",
    "                if py_file.stat().st_size > MAX_FILE_SIZE:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    content = py_file.read_text(encoding='utf-8', errors='replace')\n",
    "                    if len(content.strip()) > 50:  # Filtre fichiers vides\n",
    "                        repo_sources.append({\n",
    "                            'repo': repo_name,\n",
    "                            'path': str(py_file.relative_to(repo_path)),\n",
    "                            'type': 'python',\n",
    "                            'content': content[:5000],  # Limite contenu\n",
    "                            'size': len(content)\n",
    "                        })\n",
    "                        py_count += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Scan fichiers Markdown\n",
    "            for md_file in repo_path.rglob('*.md'):\n",
    "                if md_count >= MAX_MD_FILES_PER_REPO:\n",
    "                    break\n",
    "                    \n",
    "                if md_file.stat().st_size > MAX_FILE_SIZE:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    content = md_file.read_text(encoding='utf-8', errors='replace')\n",
    "                    if len(content.strip()) > 50:\n",
    "                        repo_sources.append({\n",
    "                            'repo': repo_name,\n",
    "                            'path': str(md_file.relative_to(repo_path)),\n",
    "                            'type': 'markdown',\n",
    "                            'content': content[:3000],  # Limite contenu MD\n",
    "                            'size': len(content)\n",
    "                        })\n",
    "                        md_count += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur scan {repo_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        all_sources.extend(repo_sources)\n",
    "        scan_stats['repos_scanned'] += 1\n",
    "        scan_stats['total_files'] += len(repo_sources)\n",
    "        scan_stats['total_size'] += sum(s['size'] for s in repo_sources)\n",
    "        \n",
    "        print(f\"  📄 {len(repo_sources)} fichiers ({py_count} .py + {md_count} .md)\")\n",
    "    \n",
    "    return all_sources, scan_stats\n",
    "\n",
    "# Exécution scan\n",
    "print(\"\\n📁 SCAN SOURCES CLOUD-OPTIMISÉ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "sources, stats = scan_sources_cloud_optimized()\n",
    "scan_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n⏱️ Scan terminé en {scan_time:.2f}s\")\n",
    "print(f\"🎯 {stats['total_files']} sources consolidées\")\n",
    "print(f\"📊 {stats['repos_scanned']} repos scannés\")\n",
    "print(f\"💾 {stats['total_size'] / 1024:.1f}KB total\")\n",
    "\n",
    "if len(sources) == 0:\n",
    "    print(\"\\n⚠️ AUCUNE SOURCE TROUVÉE\")\n",
    "    print(\"💡 Vérifiez le clonage des repos\")\n",
    "else:\n",
    "    print(f\"\\n✅ SOURCES PRÊTES POUR EMBEDDINGS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ae7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 GÉNÉRATION EMBEDDINGS CLOUD-OPTIMISÉE\n",
    "if len(sources) > 0:\n",
    "    print(\"\\n🧠 GÉNÉRATION EMBEDDINGS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        import torch\n",
    "        \n",
    "        # Configuration GPU/CPU automatique\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"🔧 Device: {device}\")\n",
    "        \n",
    "        # Modèle optimisé\n",
    "        print(\"📥 Chargement modèle...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        \n",
    "        # Préparation documents\n",
    "        documents = []\n",
    "        metadata = []\n",
    "        \n",
    "        for source in sources:\n",
    "            # Formatage document pour embeddings\n",
    "            doc_text = f\"Repo: {source['repo']}\\nFile: {source['path']}\\nType: {source['type']}\\n\\nContent:\\n{source['content']}\"\n",
    "            documents.append(doc_text)\n",
    "            metadata.append({\n",
    "                'repo': source['repo'],\n",
    "                'path': source['path'],\n",
    "                'type': source['type']\n",
    "            })\n",
    "        \n",
    "        # Limitation pour performance cloud\n",
    "        MAX_DOCS = 150  # Limite cloud\n",
    "        if len(documents) > MAX_DOCS:\n",
    "            print(f\"⚡ Limitation à {MAX_DOCS} docs pour performance cloud\")\n",
    "            documents = documents[:MAX_DOCS]\n",
    "            metadata = metadata[:MAX_DOCS]\n",
    "        \n",
    "        print(f\"🔄 Génération embeddings pour {len(documents)} documents...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Génération par batch pour éviter OOM\n",
    "        batch_size = 32 if device == 'cuda' else 16\n",
    "        embeddings = model.encode(documents, batch_size=batch_size, show_progress_bar=True)\n",
    "        \n",
    "        emb_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"✅ Embeddings générés en {emb_time:.2f}s\")\n",
    "        print(f\"📊 {len(embeddings)} vecteurs de dimension {embeddings.shape[1]}\")\n",
    "        print(f\"⚡ Performance: {len(documents)/emb_time:.1f} docs/sec\")\n",
    "        \n",
    "        embeddings_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur embeddings: {e}\")\n",
    "        embeddings_ready = False\n",
    "else:\n",
    "    print(\"⚠️ Pas de sources - skip embeddings\")\n",
    "    embeddings_ready = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af985505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔎 RECHERCHE SÉMANTIQUE INTERACTIVE\n",
    "if embeddings_ready:\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    def semantic_search_cloud(query, top_k=5):\n",
    "        \"\"\"Recherche sémantique optimisée cloud\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Génération embedding query\n",
    "            query_embedding = model.encode([query])\n",
    "            \n",
    "            # Calcul similarité\n",
    "            similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "            \n",
    "            # Top résultats\n",
    "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "            \n",
    "            results = []\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                score = similarities[idx]\n",
    "                meta = metadata[idx]\n",
    "                doc = documents[idx]\n",
    "                \n",
    "                results.append({\n",
    "                    'rank': i + 1,\n",
    "                    'score': float(score),\n",
    "                    'repo': meta['repo'],\n",
    "                    'path': meta['path'],\n",
    "                    'type': meta['type'],\n",
    "                    'content_preview': doc[:300] + '...' if len(doc) > 300 else doc\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur recherche: {e}\")\n",
    "            return []\n",
    "    \n",
    "    # Interface de recherche\n",
    "    print(\"\\n🔎 RECHERCHE SÉMANTIQUE INTERACTIVE\")\n",
    "    print(\"=\" * 35)\n",
    "    print(\"🎯 Exemples de requêtes:\")\n",
    "    print(\"  - 'filesystem implementation'\")\n",
    "    print(\"  - 'neural network training'\")\n",
    "    print(\"  - 'configuration files'\")\n",
    "    print(\"  - 'error handling'\")\n",
    "    \n",
    "    # Test automatique\n",
    "    test_query = \"filesystem implementation\"\n",
    "    print(f\"\\n🧪 Test automatique: '{test_query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = semantic_search_cloud(test_query, top_k=3)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"⚡ Recherche en {search_time:.3f}s\")\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n📊 RÉSULTATS:\")\n",
    "        for result in results:\n",
    "            print(f\"\\n{result['rank']}. 📁 {result['repo']}/{result['path']}\")\n",
    "            print(f\"   🎯 Score: {result['score']:.3f} | Type: {result['type']}\")\n",
    "            print(f\"   📝 {result['content_preview'][:150]}...\")\n",
    "    \n",
    "    print(\"\\n✅ SYSTÈME PRÊT POUR RECHERCHE INTERACTIVE\")\n",
    "    search_function_ready = True\n",
    "else:\n",
    "    search_function_ready = False\n",
    "    print(\"⚠️ Recherche non disponible - problème embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 RAPPORT FINAL CLOUD AUTONOME\n",
    "print(\"\\n🎉 RAPPORT FINAL - MODE CLOUD AUTONOME\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "report = {\n",
    "    'mode': 'CLOUD' if IS_CLOUD else 'LOCAL',\n",
    "    'repos_clones': len(available_repos) if 'available_repos' in locals() else 0,\n",
    "    'sources_scannees': len(sources) if 'sources' in locals() else 0,\n",
    "    'embeddings_generes': len(embeddings) if 'embeddings_ready' else 0,\n",
    "    'recherche_active': search_function_ready if 'search_function_ready' in locals() else False,\n",
    "    'performance': {\n",
    "        'clonage': f\"{clone_time:.2f}s\" if 'clone_time' in locals() else 'N/A',\n",
    "        'scan': f\"{scan_time:.2f}s\" if 'scan_time' in locals() else 'N/A',\n",
    "        'embeddings': f\"{emb_time:.2f}s\" if 'emb_time' in locals() else 'N/A',\n",
    "        'recherche': f\"{search_time:.3f}s\" if 'search_time' in locals() else 'N/A'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"🌍 Mode: {report['mode']}\")\n",
    "print(f\"📦 Repos clonés: {report['repos_clones']}\")\n",
    "print(f\"📄 Sources scannées: {report['sources_scannees']}\")\n",
    "print(f\"🧠 Embeddings générés: {report['embeddings_generes']}\")\n",
    "print(f\"🔎 Recherche: {'✅ ACTIVE' if report['recherche_active'] else '❌ INACTIVE'}\")\n",
    "\n",
    "print(\"\\n⚡ PERFORMANCE:\")\n",
    "for step, time_val in report['performance'].items():\n",
    "    print(f\"  {step.capitalize()}: {time_val}\")\n",
    "\n",
    "# Calcul temps total\n",
    "total_time = 0\n",
    "if 'clone_time' in locals(): total_time += clone_time\n",
    "if 'scan_time' in locals(): total_time += scan_time\n",
    "if 'emb_time' in locals(): total_time += emb_time\n",
    "\n",
    "print(f\"\\n🏁 TEMPS TOTAL: {total_time:.2f}s\")\n",
    "\n",
    "if report['recherche_active']:\n",
    "    print(\"\\n🎯 SYSTÈME 100% OPÉRATIONNEL\")\n",
    "    print(\"💡 Utilisez: semantic_search_cloud('votre requête')\")\n",
    "else:\n",
    "    print(\"\\n⚠️ SYSTÈME PARTIELLEMENT OPÉRATIONNEL\")\n",
    "    print(\"💡 Vérifiez les étapes précédentes\")\n",
    "\n",
    "print(\"\\n🚀 MODE CLOUD AUTONOME COMPLÉTÉ !\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c19084",
   "metadata": {},
   "source": [
    "# 🎯 Utilisation Interactive\n",
    "\n",
    "## Recherche Personnalisée\n",
    "```python\n",
    "# Exemples de recherches\n",
    "results = semantic_search_cloud(\"neural network training\", top_k=5)\n",
    "results = semantic_search_cloud(\"configuration files\", top_k=3)\n",
    "results = semantic_search_cloud(\"error handling patterns\", top_k=5)\n",
    "```\n",
    "\n",
    "## Exploration des Repos\n",
    "```python\n",
    "# Voir les repos disponibles\n",
    "print(\"Repos disponibles:\", available_repos)\n",
    "\n",
    "# Statistiques par repo\n",
    "repo_stats = {}\n",
    "for source in sources:\n",
    "    repo = source['repo']\n",
    "    if repo not in repo_stats:\n",
    "        repo_stats[repo] = {'python': 0, 'markdown': 0}\n",
    "    repo_stats[repo][source['type']] += 1\n",
    "\n",
    "for repo, stats in repo_stats.items():\n",
    "    print(f\"{repo}: {stats['python']} .py + {stats['markdown']} .md\")\n",
    "```\n",
    "\n",
    "**✅ Système Cloud Autonome Opérationnel !**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
