{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🌍 PRIMITIVES SÉMANTIQUES PUBLIQUES - Universelles et Réutilisables\n",
        "\"\"\"\n",
        "Principe Fondamental: Les primitives sémantiques doivent être PUBLIQUES\n",
        "- Concepts universels indépendants des données privées\n",
        "- Réutilisables dans tout contexte\n",
        "- Généralisables au monde réel\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# ===============================================\n",
        "# 🔧 PRIMITIVE: Détection Environnement Universel\n",
        "# ===============================================\n",
        "\n",
        "def detect_environment():\n",
        "    \"\"\"\n",
        "    Primitive publique: Détection universelle d'environnement\n",
        "    Retourne un contexte normalisé utilisable partout\n",
        "    \"\"\"\n",
        "    env_context = {\n",
        "        'platform': 'cloud' if any(indicator in str(os.environ) for indicator in ['colab', 'kaggle', 'paperspace']) else 'local',\n",
        "        'gpu_available': False,\n",
        "        'base_path': Path('/content') if 'google.colab' in sys.modules else Path.cwd(),\n",
        "        'capabilities': [],\n",
        "        'limitations': []\n",
        "    }\n",
        "    \n",
        "    # Détection GPU universelle\n",
        "    try:\n",
        "        import torch\n",
        "        env_context['gpu_available'] = torch.cuda.is_available()\n",
        "        env_context['capabilities'].append('pytorch')\n",
        "    except ImportError:\n",
        "        env_context['limitations'].append('pytorch_missing')\n",
        "    \n",
        "    # Détection capacités réseau\n",
        "    try:\n",
        "        subprocess.run(['ping', '-c', '1', 'github.com'], \n",
        "                      capture_output=True, timeout=5, check=True)\n",
        "        env_context['capabilities'].append('network_access')\n",
        "    except:\n",
        "        env_context['limitations'].append('network_limited')\n",
        "    \n",
        "    # Capacités système\n",
        "    if env_context['platform'] == 'cloud':\n",
        "        env_context['capabilities'].extend(['git', 'pip', 'temporary_storage'])\n",
        "        env_context['limitations'].extend(['no_persistent_storage', 'session_timeout'])\n",
        "    else:\n",
        "        env_context['capabilities'].extend(['persistent_storage', 'local_files'])\n",
        "    \n",
        "    return env_context\n",
        "\n",
        "# ===============================================\n",
        "# 🔧 PRIMITIVE: Gestion Repos Publics Universelle  \n",
        "# ===============================================\n",
        "\n",
        "def get_public_repo_sources(github_user=None, repo_patterns=None):\n",
        "    \"\"\"\n",
        "    Primitive publique: Accès aux sources de repos publics\n",
        "    Concepts universels: clonage, scanning, indexation\n",
        "    \"\"\"\n",
        "    \n",
        "    # Configuration par défaut - concepts publics\n",
        "    default_repos = [\n",
        "        {\n",
        "            'name': 'main-project',\n",
        "            'patterns': ['*.py', '*.md', '*.rst', '*.txt'],\n",
        "            'priority_dirs': ['src', 'lib', 'core', 'docs'],\n",
        "            'max_files': 50\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Si utilisateur spécifique fourni\n",
        "    if github_user and repo_patterns:\n",
        "        repo_configs = []\n",
        "        for pattern in repo_patterns:\n",
        "            repo_configs.append({\n",
        "                'name': pattern.split('/')[-1],\n",
        "                'url': f'https://github.com/{github_user}/{pattern}.git',\n",
        "                'patterns': ['*.py', '*.md'],\n",
        "                'max_files': 30\n",
        "            })\n",
        "    else:\n",
        "        # Mode générique - pas de dépendance aux données privées\n",
        "        repo_configs = default_repos\n",
        "    \n",
        "    return repo_configs\n",
        "\n",
        "# ===============================================  \n",
        "# 🔧 PRIMITIVE: Extraction Sémantique Universelle\n",
        "# ===============================================\n",
        "\n",
        "def extract_semantic_primitives(content, content_type='text'):\n",
        "    \"\"\"\n",
        "    Primitive publique: Extraction de concepts sémantiques universels\n",
        "    Indépendant du domaine spécifique\n",
        "    \"\"\"\n",
        "    \n",
        "    semantic_features = {\n",
        "        'concepts': [],\n",
        "        'patterns': [],\n",
        "        'relationships': [],\n",
        "        'abstractions': [],\n",
        "        'metadata': {\n",
        "            'language': 'unknown',\n",
        "            'complexity': 'simple',\n",
        "            'domain': 'general'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Analyse universelle du contenu\n",
        "    lines = content.split('\\n')\n",
        "    words = content.lower().split()\n",
        "    \n",
        "    # Détection concepts universels\n",
        "    universal_concepts = {\n",
        "        'data_structures': ['list', 'dict', 'array', 'tree', 'graph', 'table'],\n",
        "        'algorithms': ['sort', 'search', 'filter', 'map', 'reduce', 'iterate'],\n",
        "        'patterns': ['class', 'function', 'method', 'interface', 'module'],\n",
        "        'operations': ['create', 'read', 'update', 'delete', 'process', 'transform'],\n",
        "        'abstractions': ['model', 'service', 'controller', 'manager', 'handler']\n",
        "    }\n",
        "    \n",
        "    for category, keywords in universal_concepts.items():\n",
        "        found_concepts = [kw for kw in keywords if kw in words]\n",
        "        if found_concepts:\n",
        "            semantic_features['concepts'].extend([(category, concept) for concept in found_concepts])\n",
        "    \n",
        "    # Détection patterns de code universels\n",
        "    if content_type == 'code':\n",
        "        if 'class ' in content:\n",
        "            semantic_features['patterns'].append('object_oriented')\n",
        "        if 'def ' in content or 'function' in content:\n",
        "            semantic_features['patterns'].append('functional')\n",
        "        if 'import ' in content:\n",
        "            semantic_features['patterns'].append('modular')\n",
        "    \n",
        "    # Calcul complexité universelle\n",
        "    complexity_score = len(lines) * 0.1 + len(words) * 0.01 + content.count('{') * 0.5\n",
        "    \n",
        "    if complexity_score > 100:\n",
        "        semantic_features['metadata']['complexity'] = 'complex'\n",
        "    elif complexity_score > 50:\n",
        "        semantic_features['metadata']['complexity'] = 'moderate'\n",
        "    \n",
        "    return semantic_features\n",
        "\n",
        "# Initialisation\n",
        "print(\"🌍 PRIMITIVES SÉMANTIQUES PUBLIQUES INITIALISÉES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "env = detect_environment()\n",
        "print(f\"🔧 Environnement: {env['platform']}\")\n",
        "print(f\"⚡ GPU: {'✅' if env['gpu_available'] else '❌'}\")\n",
        "print(f\"📁 Base: {env['base_path']}\")\n",
        "print(f\"🚀 Capacités: {', '.join(env['capabilities'])}\")\n",
        "if env['limitations']:\n",
        "    print(f\"⚠️ Limitations: {', '.join(env['limitations'])}\")\n",
        "\n",
        "print(\"\\n✅ Système prêt pour traitement sémantique universel\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Système de Progression pour Travaux de Longue Haleine\n",
        "\n",
        "## 🎯 Fonctionnalités de Suivi\n",
        "- **Barres de progression visuelles** : Pour chaque étape longue\n",
        "- **Estimations de temps** : Temps restant en temps réel\n",
        "- **Indicateurs d'état** : Phase actuelle, sous-tâches\n",
        "- **Logging détaillé** : Journalisation des opérations\n",
        "- **Points de sauvegarde** : Possibilité de reprendre le travail\n",
        "- **Métriques de performance** : Vitesse de traitement, statistiques\n",
        "\n",
        "## 📊 Types de Progression Supportés\n",
        "1. **Clonage de repos** : Progression par repo avec estimation\n",
        "2. **Scan de fichiers** : Compteurs temps réel avec ETA\n",
        "3. **Génération d'embeddings** : Barres par batch avec métriques\n",
        "4. **Recherche sémantique** : Indicateurs de traitement\n",
        "5. **Clustering** : Progression des calculs ML\n",
        "\n",
        "## 🔧 Outils de Monitoring\n",
        "- `tqdm` : Barres de progression élégantes\n",
        "- `time` : Mesures de performance\n",
        "- `logging` : Journalisation structurée\n",
        "- `IPython.display` : Affichage dynamique\n",
        "- `threading` : Tâches en arrière-plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \udd0d PRIMITIVE: Découverte Sémantique Universelle\n",
        "\"\"\"\n",
        "Concept Public: Découverte automatique de patterns dans n'importe quel corpus\n",
        "Généralisation: Applicable à tout domaine (code, docs, données)\n",
        "\"\"\"\n",
        "\n",
        "def discover_semantic_landscape(sources, discovery_mode='adaptive'):\n",
        "    \"\"\"\n",
        "    Primitive publique: Cartographie sémantique universelle\n",
        "    - Indépendante du domaine spécifique\n",
        "    - Réutilisable pour tout corpus\n",
        "    - Concepts transférables\n",
        "    \"\"\"\n",
        "    \n",
        "    landscape = {\n",
        "        'domains': {},\n",
        "        'patterns': {},\n",
        "        'clusters': {},\n",
        "        'relationships': [],\n",
        "        'universals': {\n",
        "            'information_architecture': [],\n",
        "            'behavioral_patterns': [],\n",
        "            'structural_patterns': [],\n",
        "            'conceptual_hierarchies': []\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"🔍 Découverte sémantique en mode {discovery_mode}\")\n",
        "    print(f\"📊 Analyse de {len(sources)} sources\")\n",
        "    \n",
        "    # ===============================================\n",
        "    # Analyse des Domaines Universels\n",
        "    # ===============================================\n",
        "    \n",
        "    domain_indicators = {\n",
        "        'technical': ['code', 'function', 'class', 'algorithm', 'system'],\n",
        "        'documentation': ['guide', 'tutorial', 'readme', 'documentation', 'manual'],\n",
        "        'configuration': ['config', 'settings', 'parameters', 'options', 'preferences'],\n",
        "        'process': ['workflow', 'pipeline', 'process', 'procedure', 'method'],\n",
        "        'data': ['model', 'schema', 'structure', 'format', 'database'],\n",
        "        'interface': ['api', 'interface', 'endpoint', 'service', 'client']\n",
        "    }\n",
        "    \n",
        "    for source in sources:\n",
        "        content_lower = source.get('content', '').lower()\n",
        "        source_domains = []\n",
        "        \n",
        "        for domain, indicators in domain_indicators.items():\n",
        "            score = sum(content_lower.count(indicator) for indicator in indicators)\n",
        "            if score > 0:\n",
        "                source_domains.append((domain, score))\n",
        "        \n",
        "        # Attribution domaine principal\n",
        "        if source_domains:\n",
        "            primary_domain = max(source_domains, key=lambda x: x[1])[0]\n",
        "            if primary_domain not in landscape['domains']:\n",
        "                landscape['domains'][primary_domain] = []\n",
        "            landscape['domains'][primary_domain].append(source)\n",
        "    \n",
        "    # ===============================================\n",
        "    # Détection Patterns Structurels Universels\n",
        "    # ===============================================\n",
        "    \n",
        "    structural_patterns = {\n",
        "        'hierarchical': lambda c: c.count('    ') > 5,  # Indentation\n",
        "        'sequential': lambda c: len([l for l in c.split('\\n') if l.strip().startswith(('1.', '2.', '-', '*'))]) > 3,\n",
        "        'networked': lambda c: c.count('->') + c.count('<-') + c.count('link') > 2,\n",
        "        'modular': lambda c: c.count('import') + c.count('include') + c.count('require') > 2,\n",
        "        'layered': lambda c: any(layer in c.lower() for layer in ['layer', 'tier', 'level', 'stack']),\n",
        "        'event_driven': lambda c: any(event in c.lower() for event in ['event', 'trigger', 'handler', 'callback'])\n",
        "    }\n",
        "    \n",
        "    for pattern_name, detector in structural_patterns.items():\n",
        "        matching_sources = [s for s in sources if detector(s.get('content', ''))]\n",
        "        if matching_sources:\n",
        "            landscape['patterns'][pattern_name] = {\n",
        "                'count': len(matching_sources),\n",
        "                'examples': matching_sources[:3],\n",
        "                'coverage': len(matching_sources) / len(sources)\n",
        "            }\n",
        "    \n",
        "    # ===============================================\n",
        "    # Identification Universels Transférables  \n",
        "    # ===============================================\n",
        "    \n",
        "    # Architectures d'information universelles\n",
        "    info_arch_patterns = []\n",
        "    for domain, domain_sources in landscape['domains'].items():\n",
        "        if len(domain_sources) > 3:\n",
        "            info_arch_patterns.append({\n",
        "                'domain': domain,\n",
        "                'organization': 'clustered',\n",
        "                'size': len(domain_sources),\n",
        "                'transferable_concepts': extract_transferable_concepts(domain_sources)\n",
        "            })\n",
        "    \n",
        "    landscape['universals']['information_architecture'] = info_arch_patterns\n",
        "    \n",
        "    # Patterns comportementaux universels\n",
        "    behavioral_indicators = {\n",
        "        'initialization': ['setup', 'init', 'configure', 'prepare'],\n",
        "        'processing': ['process', 'transform', 'handle', 'execute'],\n",
        "        'validation': ['validate', 'check', 'verify', 'test'],\n",
        "        'cleanup': ['cleanup', 'close', 'finalize', 'destroy']\n",
        "    }\n",
        "    \n",
        "    behavior_patterns = {}\n",
        "    for behavior, indicators in behavioral_indicators.items():\n",
        "        count = sum(sum(source.get('content', '').lower().count(ind) for ind in indicators) for source in sources)\n",
        "        if count > 0:\n",
        "            behavior_patterns[behavior] = count\n",
        "    \n",
        "    landscape['universals']['behavioral_patterns'] = behavior_patterns\n",
        "    \n",
        "    return landscape\n",
        "\n",
        "def extract_transferable_concepts(sources):\n",
        "    \"\"\"Extraction de concepts réutilisables dans d'autres domaines\"\"\"\n",
        "    \n",
        "    concepts = {\n",
        "        'abstractions': set(),\n",
        "        'patterns': set(), \n",
        "        'principles': set()\n",
        "    }\n",
        "    \n",
        "    # Analyse des abstractions communes\n",
        "    common_abstractions = ['manager', 'handler', 'processor', 'controller', 'service', 'adapter']\n",
        "    \n",
        "    for source in sources:\n",
        "        content = source.get('content', '').lower()\n",
        "        for abstraction in common_abstractions:\n",
        "            if abstraction in content:\n",
        "                concepts['abstractions'].add(abstraction)\n",
        "    \n",
        "    # Patterns de nommage transférables\n",
        "    naming_patterns = ['create_', 'get_', 'set_', 'is_', 'has_', 'can_', 'should_']\n",
        "    for source in sources:\n",
        "        content = source.get('content', '')\n",
        "        for pattern in naming_patterns:\n",
        "            if pattern in content:\n",
        "                concepts['patterns'].add(pattern.rstrip('_') + '_pattern')\n",
        "    \n",
        "    return {k: list(v) for k, v in concepts.items()}\n",
        "\n",
        "# Test de découverte avec données exemple\n",
        "print(\"🧪 Test découverte sémantique universelle...\")\n",
        "\n",
        "# Données exemple universelles (pas spécifiques à un projet)\n",
        "example_sources = [\n",
        "    {'content': 'class DataProcessor:\\n    def process(self, data):\\n        return self.transform(data)', 'type': 'code'},\n",
        "    {'content': '# Configuration Guide\\n\\nThis guide explains how to configure the system parameters.', 'type': 'docs'},\n",
        "    {'content': 'def validate_input(data):\\n    if not data:\\n        raise ValueError(\"Invalid input\")', 'type': 'code'},\n",
        "    {'content': 'API Endpoints:\\n- GET /api/data\\n- POST /api/process', 'type': 'docs'}\n",
        "]\n",
        "\n",
        "landscape = discover_semantic_landscape(example_sources)\n",
        "\n",
        "print(\"\\\\n📊 PAYSAGE SÉMANTIQUE DÉCOUVERT:\")\n",
        "print(f\"🎯 Domaines identifiés: {list(landscape['domains'].keys())}\")\n",
        "print(f\"🔄 Patterns structurels: {list(landscape['patterns'].keys())}\")\n",
        "print(f\"🌍 Concepts universels transférables: {len(landscape['universals']['information_architecture'])}\")\n",
        "\n",
        "print(\"\\\\n✅ Primitive de découverte opérationnelle\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 UTILITAIRES DE PROGRESSION - Travaux de Longue Haleine\n",
        "\"\"\"\n",
        "Système de progression visuelle pour les opérations longues\n",
        "Principe: Feedback utilisateur constant avec estimations temps réel\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "    TQDM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TQDM_AVAILABLE = False\n",
        "    print(\"⚠️ tqdm non disponible - barres de progression simplifiées\")\n",
        "\n",
        "try:\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    IPYTHON_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IPYTHON_AVAILABLE = False\n",
        "\n",
        "class ProgressTracker:\n",
        "    \"\"\"\n",
        "    Gestionnaire de progression universel pour travaux de longue haleine\n",
        "    - Barres de progression visuelles\n",
        "    - Estimations temps restant\n",
        "    - Logging détaillé\n",
        "    - Sauvegarde d'état\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_name=\"Traitement\", enable_logging=True):\n",
        "        self.task_name = task_name\n",
        "        self.enable_logging = enable_logging\n",
        "        self.start_time = None\n",
        "        self.phases = {}\n",
        "        self.current_phase = None\n",
        "        self.global_progress = 0\n",
        "        self.global_total = 100\n",
        "        self.log_messages = []\n",
        "        self.performance_metrics = defaultdict(list)\n",
        "        \n",
        "    def start_task(self, total_phases=None):\n",
        "        \"\"\"Démarrage du suivi de tâche\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        if total_phases:\n",
        "            self.global_total = total_phases * 100\n",
        "        \n",
        "        self._log(f\"🚀 Démarrage: {self.task_name}\")\n",
        "        if IPYTHON_AVAILABLE:\n",
        "            self._display_header()\n",
        "    \n",
        "    def start_phase(self, phase_name, total_items=None, description=\"\"):\n",
        "        \"\"\"Démarrage d'une nouvelle phase\"\"\"\n",
        "        self.current_phase = phase_name\n",
        "        \n",
        "        phase_info = {\n",
        "            'name': phase_name,\n",
        "            'description': description,\n",
        "            'start_time': time.time(),\n",
        "            'total_items': total_items,\n",
        "            'completed_items': 0,\n",
        "            'progress_bar': None,\n",
        "            'estimated_remaining': None\n",
        "        }\n",
        "        \n",
        "        self.phases[phase_name] = phase_info\n",
        "        \n",
        "        # Création barre de progression si tqdm disponible\n",
        "        if TQDM_AVAILABLE and total_items:\n",
        "            phase_info['progress_bar'] = tqdm(\n",
        "                total=total_items,\n",
        "                desc=f\"📋 {phase_name}\",\n",
        "                unit=\"items\",\n",
        "                leave=True,\n",
        "                ncols=100\n",
        "            )\n",
        "        \n",
        "        self._log(f\"📋 Phase: {phase_name}\" + (f\" - {description}\" if description else \"\"))\n",
        "        self._update_display()\n",
        "    \n",
        "    def update_progress(self, phase_name=None, increment=1, custom_message=\"\"):\n",
        "        \"\"\"Mise à jour de la progression\"\"\"\n",
        "        if not phase_name:\n",
        "            phase_name = self.current_phase\n",
        "            \n",
        "        if phase_name not in self.phases:\n",
        "            return\n",
        "        \n",
        "        phase = self.phases[phase_name]\n",
        "        phase['completed_items'] += increment\n",
        "        \n",
        "        # Mise à jour barre tqdm\n",
        "        if phase['progress_bar']:\n",
        "            phase['progress_bar'].update(increment)\n",
        "            if custom_message:\n",
        "                phase['progress_bar'].set_postfix_str(custom_message)\n",
        "        \n",
        "        # Calcul estimation temps restant\n",
        "        if phase['total_items'] and phase['completed_items'] > 0:\n",
        "            elapsed = time.time() - phase['start_time']\n",
        "            rate = phase['completed_items'] / elapsed\n",
        "            remaining_items = phase['total_items'] - phase['completed_items']\n",
        "            estimated_remaining = remaining_items / rate if rate > 0 else None\n",
        "            phase['estimated_remaining'] = estimated_remaining\n",
        "        \n",
        "        # Log périodique\n",
        "        if phase['completed_items'] % max(1, (phase['total_items'] or 10) // 10) == 0:\n",
        "            percentage = (phase['completed_items'] / (phase['total_items'] or 1)) * 100\n",
        "            eta_str = \"\"\n",
        "            if phase['estimated_remaining']:\n",
        "                eta = timedelta(seconds=int(phase['estimated_remaining']))\n",
        "                eta_str = f\" - ETA: {eta}\"\n",
        "            \n",
        "            self._log(f\"  ⏳ {phase_name}: {phase['completed_items']}/{phase['total_items'] or '?'} ({percentage:.1f}%){eta_str}\")\n",
        "        \n",
        "        self._update_display()\n",
        "    \n",
        "    def finish_phase(self, phase_name=None, success=True):\n",
        "        \"\"\"Finalisation d'une phase\"\"\"\n",
        "        if not phase_name:\n",
        "            phase_name = self.current_phase\n",
        "            \n",
        "        if phase_name not in self.phases:\n",
        "            return\n",
        "        \n",
        "        phase = self.phases[phase_name]\n",
        "        phase['end_time'] = time.time()\n",
        "        phase['duration'] = phase['end_time'] - phase['start_time']\n",
        "        \n",
        "        # Fermeture barre de progression\n",
        "        if phase['progress_bar']:\n",
        "            phase['progress_bar'].close()\n",
        "        \n",
        "        status = \"✅\" if success else \"❌\"\n",
        "        duration_str = f\"{phase['duration']:.2f}s\"\n",
        "        \n",
        "        if phase['total_items']:\n",
        "            rate = phase['completed_items'] / phase['duration']\n",
        "            self.performance_metrics[phase_name].append(rate)\n",
        "            self._log(f\"{status} {phase_name} complété en {duration_str} ({rate:.1f} items/s)\")\n",
        "        else:\n",
        "            self._log(f\"{status} {phase_name} complété en {duration_str}\")\n",
        "        \n",
        "        self.global_progress += 100 / len(self.phases) if self.phases else 0\n",
        "        self._update_display()\n",
        "    \n",
        "    def finish_task(self):\n",
        "        \"\"\"Finalisation complète de la tâche\"\"\"\n",
        "        if self.start_time:\n",
        "            total_duration = time.time() - self.start_time\n",
        "            self._log(f\"🎉 {self.task_name} terminé en {total_duration:.2f}s\")\n",
        "            \n",
        "            # Résumé des performances\n",
        "            if self.performance_metrics:\n",
        "                self._log(\"📊 Résumé des performances:\")\n",
        "                for phase, rates in self.performance_metrics.items():\n",
        "                    avg_rate = sum(rates) / len(rates)\n",
        "                    self._log(f\"  • {phase}: {avg_rate:.1f} items/s moyen\")\n",
        "        \n",
        "        self._update_display(final=True)\n",
        "    \n",
        "    def _log(self, message):\n",
        "        \"\"\"Logging avec timestamp\"\"\"\n",
        "        if self.enable_logging:\n",
        "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            formatted_message = f\"[{timestamp}] {message}\"\n",
        "            self.log_messages.append(formatted_message)\n",
        "            print(formatted_message)\n",
        "    \n",
        "    def _display_header(self):\n",
        "        \"\"\"Affichage de l'en-tête de progression\"\"\"\n",
        "        if not IPYTHON_AVAILABLE:\n",
        "            return\n",
        "        \n",
        "        header_html = f\"\"\"\n",
        "        <div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 5px; background: #f9f9f9;\">\n",
        "            <h3>🚀 {self.task_name} - Progression en Temps Réel</h3>\n",
        "            <div id=\"progress-status\">Initialisation...</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(header_html))\n",
        "    \n",
        "    def _update_display(self, final=False):\n",
        "        \"\"\"Mise à jour de l'affichage dynamique\"\"\"\n",
        "        if not IPYTHON_AVAILABLE:\n",
        "            return\n",
        "        \n",
        "        # Construction de l'affichage de statut\n",
        "        if self.start_time:\n",
        "            elapsed = time.time() - self.start_time\n",
        "            elapsed_str = str(timedelta(seconds=int(elapsed)))\n",
        "        else:\n",
        "            elapsed_str = \"00:00:00\"\n",
        "        \n",
        "        status_parts = [f\"⏱️ Temps écoulé: {elapsed_str}\"]\n",
        "        \n",
        "        if self.current_phase and self.current_phase in self.phases:\n",
        "            phase = self.phases[self.current_phase]\n",
        "            phase_progress = \"\"\n",
        "            \n",
        "            if phase['total_items']:\n",
        "                percentage = (phase['completed_items'] / phase['total_items']) * 100\n",
        "                phase_progress = f\"({percentage:.1f}%)\"\n",
        "            \n",
        "            status_parts.append(f\"📋 Phase actuelle: {self.current_phase} {phase_progress}\")\n",
        "            \n",
        "            if phase['estimated_remaining']:\n",
        "                eta = timedelta(seconds=int(phase['estimated_remaining']))\n",
        "                status_parts.append(f\"⏳ ETA phase: {eta}\")\n",
        "        \n",
        "        status_html = f\"\"\"\n",
        "        <div style=\"padding: 5px; background: #e8f5e8; border-radius: 3px;\">\n",
        "            {' | '.join(status_parts)}\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        \n",
        "        # Affichage final ou mise à jour\n",
        "        if final:\n",
        "            final_html = f\"\"\"\n",
        "            <div style=\"border: 2px solid #4CAF50; padding: 10px; border-radius: 5px; background: #e8f5e8;\">\n",
        "                <h3>✅ {self.task_name} - Terminé avec Succès</h3>\n",
        "                {status_html}\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(final_html))\n",
        "\n",
        "# Fonctions utilitaires pour progression simple\n",
        "\n",
        "def progress_bar_simple(iterable, description=\"Traitement\"):\n",
        "    \"\"\"Barre de progression simple si tqdm non disponible\"\"\"\n",
        "    if TQDM_AVAILABLE:\n",
        "        return tqdm(iterable, desc=description, ncols=100)\n",
        "    else:\n",
        "        # Fallback simple avec compteur\n",
        "        total = len(list(iterable)) if hasattr(iterable, '__len__') else None\n",
        "        \n",
        "        class SimpleProgress:\n",
        "            def __init__(self, iterable, total=None):\n",
        "                self.iterable = iterable\n",
        "                self.total = total\n",
        "                self.count = 0\n",
        "                \n",
        "            def __iter__(self):\n",
        "                for item in self.iterable:\n",
        "                    self.count += 1\n",
        "                    if self.total:\n",
        "                        percentage = (self.count / self.total) * 100\n",
        "                        print(f\"\\r{description}: {self.count}/{self.total} ({percentage:.1f}%)\", end=\"\", flush=True)\n",
        "                    else:\n",
        "                        print(f\"\\r{description}: {self.count} items\", end=\"\", flush=True)\n",
        "                    yield item\n",
        "                print()  # Nouvelle ligne à la fin\n",
        "        \n",
        "        return SimpleProgress(iterable, total)\n",
        "\n",
        "def estimate_time_remaining(start_time, current_progress, total_progress):\n",
        "    \"\"\"Estimation simple du temps restant\"\"\"\n",
        "    if current_progress <= 0:\n",
        "        return None\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    rate = current_progress / elapsed\n",
        "    remaining = total_progress - current_progress\n",
        "    \n",
        "    return remaining / rate if rate > 0 else None\n",
        "\n",
        "# Test des utilitaires de progression\n",
        "print(\"📊 SYSTÈME DE PROGRESSION INITIALISÉ\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Démonstration simple\n",
        "print(\"🧪 Test progression simple...\")\n",
        "demo_tracker = ProgressTracker(\"Démonstration Progression\")\n",
        "demo_tracker.start_task()\n",
        "\n",
        "demo_tracker.start_phase(\"Test rapide\", total_items=5, description=\"Démonstration des capacités\")\n",
        "for i in range(5):\n",
        "    time.sleep(0.1)  # Simulation travail\n",
        "    demo_tracker.update_progress(custom_message=f\"Item {i+1}\")\n",
        "\n",
        "demo_tracker.finish_phase()\n",
        "demo_tracker.finish_task()\n",
        "\n",
        "print(\"\\n✅ Système de progression opérationnel pour travaux de longue haleine\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🎯 PRIMITIVE: Recherche Sémantique Universelle avec Progression\n",
        "\"\"\"\n",
        "Concept Public: Moteur de recherche sémantique générique avec suivi temps réel\n",
        "Réutilisable: Pour tout corpus, tout domaine, toute langue\n",
        "Transférable: Patterns applicables partout\n",
        "NOUVEAU: Progression visuelle pour travaux de longue haleine\n",
        "\"\"\"\n",
        "\n",
        "class UniversalSemanticSearch:\n",
        "    \"\"\"\n",
        "    Primitive publique: Recherche sémantique universelle avec progression\n",
        "    - Indépendante du domaine d'application\n",
        "    - Réutilisable pour tout type de contenu\n",
        "    - Concepts transférables à d'autres contextes\n",
        "    - Suivi de progression pour opérations longues\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', enable_progress=True):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.embeddings = None\n",
        "        self.documents = []\n",
        "        self.metadata = []\n",
        "        self.semantic_clusters = {}\n",
        "        self.enable_progress = enable_progress\n",
        "        self.progress_tracker = None\n",
        "        \n",
        "    def initialize_engine(self):\n",
        "        \"\"\"Initialisation universelle du moteur sémantique avec progression\"\"\"\n",
        "        \n",
        "        if self.enable_progress:\n",
        "            self.progress_tracker = ProgressTracker(\"Moteur Sémantique Universel\")\n",
        "            self.progress_tracker.start_task(total_phases=3)\n",
        "            self.progress_tracker.start_phase(\"Initialisation\", total_items=2, description=\"Chargement modèle et dépendances\")\n",
        "        \n",
        "        try:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            \n",
        "            if self.progress_tracker:\n",
        "                self.progress_tracker.update_progress(custom_message=\"Import sentence-transformers\")\n",
        "            \n",
        "            print(f\"🔧 Initialisation moteur sémantique: {self.model_name}\")\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "            \n",
        "            if self.progress_tracker:\n",
        "                self.progress_tracker.update_progress(custom_message=\"Modèle chargé\")\n",
        "                self.progress_tracker.finish_phase(success=True)\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except ImportError:\n",
        "            print(\"❌ sentence-transformers non disponible\")\n",
        "            print(\"💡 Installation: pip install sentence-transformers\")\n",
        "            if self.progress_tracker:\n",
        "                self.progress_tracker.finish_phase(success=False)\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur initialisation: {e}\")\n",
        "            if self.progress_tracker:\n",
        "                self.progress_tracker.finish_phase(success=False)\n",
        "            return False\n",
        "    \n",
        "    def index_corpus(self, sources, max_docs=100):\n",
        "        \"\"\"\n",
        "        Indexation universelle de corpus avec progression temps réel\n",
        "        Concept transférable: preprocessing + vectorisation + monitoring\n",
        "        \"\"\"\n",
        "        \n",
        "        print(f\"📚 Indexation corpus universel ({len(sources)} sources)\")\n",
        "        \n",
        "        if not self.model:\n",
        "            if not self.initialize_engine():\n",
        "                return False\n",
        "        \n",
        "        if self.progress_tracker:\n",
        "            self.progress_tracker.start_phase(\"Préprocessing\", total_items=len(sources[:max_docs]), \n",
        "                                            description=\"Nettoyage et enrichissement contextuel\")\n",
        "        \n",
        "        # ===============================================\n",
        "        # Préprocessing Universel avec Progression\n",
        "        # ===============================================\n",
        "        \n",
        "        processed_docs = []\n",
        "        processed_metadata = []\n",
        "        \n",
        "        sources_to_process = sources[:max_docs]\n",
        "        \n",
        "        for i, source in enumerate(sources_to_process):\n",
        "            # Normalisation universelle\n",
        "            content = source.get('content', '')\n",
        "            \n",
        "            # Nettoyage universel (applicable partout)\n",
        "            content = content.replace('\\\\n\\\\n\\\\n', '\\\\n\\\\n')  # Réduction espaces\n",
        "            content = content.replace('\\\\t', '  ')  # Normalisation indentation\n",
        "            content = ' '.join(content.split())  # Normalisation espaces\n",
        "            \n",
        "            # Enrichissement contextuel universel\n",
        "            context_parts = []\n",
        "            \n",
        "            # Métadonnées universelles\n",
        "            if 'type' in source:\n",
        "                context_parts.append(f\"Type: {source['type']}\")\n",
        "            if 'domain' in source:\n",
        "                context_parts.append(f\"Domain: {source['domain']}\")\n",
        "            if 'category' in source:\n",
        "                context_parts.append(f\"Category: {source['category']}\")\n",
        "            \n",
        "            # Construction document enrichi\n",
        "            if context_parts:\n",
        "                enriched_doc = f\"[{' | '.join(context_parts)}] {content}\"\n",
        "            else:\n",
        "                enriched_doc = content\n",
        "            \n",
        "            processed_docs.append(enriched_doc)\n",
        "            processed_metadata.append({\n",
        "                'index': i,\n",
        "                'original_source': source,\n",
        "                'content_length': len(content),\n",
        "                'enrichment_applied': len(context_parts) > 0\n",
        "            })\n",
        "            \n",
        "            # Mise à jour progression\n",
        "            if self.progress_tracker:\n",
        "                progress_msg = f\"Doc {i+1}: {len(content)} chars\"\n",
        "                if len(context_parts) > 0:\n",
        "                    progress_msg += f\" (+enriched)\"\n",
        "                self.progress_tracker.update_progress(custom_message=progress_msg)\n",
        "        \n",
        "        if self.progress_tracker:\n",
        "            self.progress_tracker.finish_phase(success=True)\n",
        "        \n",
        "        # ===============================================\n",
        "        # Vectorisation Universelle avec Progression\n",
        "        # ===============================================\n",
        "        \n",
        "        if self.progress_tracker:\n",
        "            # Estimation nombre de batches pour progression\n",
        "            batch_size = 32\n",
        "            estimated_batches = (len(processed_docs) + batch_size - 1) // batch_size\n",
        "            self.progress_tracker.start_phase(\"Vectorisation\", total_items=estimated_batches,\n",
        "                                            description=\"Génération embeddings par batches\")\n",
        "        \n",
        "        print(f\"🔄 Vectorisation de {len(processed_docs)} documents...\")\n",
        "        \n",
        "        try:\n",
        "            # Vectorisation avec callback de progression custom\n",
        "            def progress_callback(batch_idx, total_batches):\n",
        "                if self.progress_tracker:\n",
        "                    self.progress_tracker.update_progress(\n",
        "                        custom_message=f\"Batch {batch_idx+1}/{total_batches}\"\n",
        "                    )\n",
        "            \n",
        "            # Vectorisation par batches avec monitoring\n",
        "            embeddings_list = []\n",
        "            batch_size = 32\n",
        "            total_batches = (len(processed_docs) + batch_size - 1) // batch_size\n",
        "            \n",
        "            for batch_idx in range(0, len(processed_docs), batch_size):\n",
        "                batch_docs = processed_docs[batch_idx:batch_idx + batch_size]\n",
        "                batch_embeddings = self.model.encode(\n",
        "                    batch_docs,\n",
        "                    batch_size=len(batch_docs),\n",
        "                    show_progress_bar=False,  # On gère notre propre progression\n",
        "                    convert_to_tensor=False,\n",
        "                    normalize_embeddings=True\n",
        "                )\n",
        "                embeddings_list.append(batch_embeddings)\n",
        "                \n",
        "                # Progression custom\n",
        "                if self.progress_tracker:\n",
        "                    current_batch = batch_idx // batch_size + 1\n",
        "                    self.progress_tracker.update_progress(\n",
        "                        custom_message=f\"Batch {current_batch}/{total_batches} - {len(batch_docs)} docs\"\n",
        "                    )\n",
        "            \n",
        "            # Concaténation des embeddings\n",
        "            import numpy as np\n",
        "            self.embeddings = np.vstack(embeddings_list)\n",
        "            \n",
        "            self.documents = processed_docs\n",
        "            self.metadata = processed_metadata\n",
        "            \n",
        "            if self.progress_tracker:\n",
        "                self.progress_tracker.finish_phase(success=True)\n",
        "                self.progress_tracker.finish_task()\n",
        "            \n",
        "            print(f\"✅ Indexation complète: {len(self.embeddings)} vecteurs\")\n",
        "            print(f\"📊 Dimension: {self.embeddings.shape[1]}\")\n",
        "            print(f\"💾 Taille: {self.embeddings.nbytes / 1024 / 1024:.2f}MB\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur vectorisation: {e}\")\n",
        "            if self.progress_tracker:\n",
        "                self.progress_tracker.finish_phase(success=False)\n",
        "            return False\n",
        "    \n",
        "    def semantic_search_with_progress(self, query, top_k=5, semantic_threshold=0.1):\n",
        "        \"\"\"\n",
        "        Recherche sémantique universelle avec progression pour requêtes complexes\n",
        "        \"\"\"\n",
        "        \n",
        "        if not self.model or self.embeddings is None:\n",
        "            print(\"❌ Moteur non initialisé\")\n",
        "            return []\n",
        "        \n",
        "        # Progression pour recherches longues\n",
        "        search_tracker = ProgressTracker(f\"Recherche: '{query[:30]}...'\") if self.enable_progress else None\n",
        "        \n",
        "        if search_tracker:\n",
        "            search_tracker.start_task(total_phases=3)\n",
        "            search_tracker.start_phase(\"Vectorisation Query\", total_items=1)\n",
        "        \n",
        "        try:\n",
        "            from sklearn.metrics.pairwise import cosine_similarity\n",
        "            import numpy as np\n",
        "            \n",
        "            # Vectorisation query universelle\n",
        "            query_embedding = self.model.encode([query], normalize_embeddings=True)\n",
        "            \n",
        "            if search_tracker:\n",
        "                search_tracker.update_progress(custom_message=\"Query vectorisée\")\n",
        "                search_tracker.finish_phase()\n",
        "                search_tracker.start_phase(\"Calcul Similarités\", total_items=len(self.embeddings))\n",
        "            \n",
        "            # Calcul similarités avec progression pour gros corpus\n",
        "            if len(self.embeddings) > 1000:\n",
        "                # Calcul par chunks pour gros corpus\n",
        "                chunk_size = 1000\n",
        "                similarities = []\n",
        "                \n",
        "                for i in range(0, len(self.embeddings), chunk_size):\n",
        "                    chunk_embeddings = self.embeddings[i:i+chunk_size]\n",
        "                    chunk_similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
        "                    similarities.extend(chunk_similarities)\n",
        "                    \n",
        "                    if search_tracker:\n",
        "                        progress = min(i + chunk_size, len(self.embeddings))\n",
        "                        for _ in range(len(chunk_similarities)):\n",
        "                            search_tracker.update_progress(custom_message=f\"Chunk {i//chunk_size + 1}\")\n",
        "                \n",
        "                similarities = np.array(similarities)\n",
        "            else:\n",
        "                # Calcul direct pour petits corpus\n",
        "                similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "                if search_tracker:\n",
        "                    for i in range(len(similarities)):\n",
        "                        search_tracker.update_progress(custom_message=f\"Doc {i+1}\")\n",
        "            \n",
        "            if search_tracker:\n",
        "                search_tracker.finish_phase()\n",
        "                search_tracker.start_phase(\"Ranking Résultats\", total_items=top_k)\n",
        "            \n",
        "            # Filtrage et ranking\n",
        "            valid_indices = np.where(similarities >= semantic_threshold)[0]\n",
        "            \n",
        "            if len(valid_indices) == 0:\n",
        "                if search_tracker:\n",
        "                    search_tracker.finish_phase()\n",
        "                    search_tracker.finish_task()\n",
        "                return {\n",
        "                    'query': query,\n",
        "                    'results': [],\n",
        "                    'stats': {'total_candidates': len(similarities), 'threshold': semantic_threshold}\n",
        "                }\n",
        "            \n",
        "            # Ranking universel\n",
        "            valid_similarities = similarities[valid_indices]\n",
        "            sorted_indices = valid_indices[np.argsort(valid_similarities)[::-1]]\n",
        "            \n",
        "            # Construction résultats avec progression\n",
        "            results = []\n",
        "            for rank, idx in enumerate(sorted_indices[:top_k]):\n",
        "                result = {\n",
        "                    'rank': rank + 1,\n",
        "                    'similarity_score': float(similarities[idx]),\n",
        "                    'semantic_strength': self._classify_semantic_strength(similarities[idx]),\n",
        "                    'document_index': int(idx),\n",
        "                    'metadata': self.metadata[idx],\n",
        "                    'content_preview': self.documents[idx][:300] + '...' if len(self.documents[idx]) > 300 else self.documents[idx]\n",
        "                }\n",
        "                results.append(result)\n",
        "                \n",
        "                if search_tracker:\n",
        "                    search_tracker.update_progress(custom_message=f\"Résultat {rank+1}\")\n",
        "            \n",
        "            if search_tracker:\n",
        "                search_tracker.finish_phase()\n",
        "                search_tracker.finish_task()\n",
        "            \n",
        "            return {\n",
        "                'query': query,\n",
        "                'results': results,\n",
        "                'stats': {\n",
        "                    'total_candidates': len(similarities),\n",
        "                    'valid_candidates': len(valid_indices),\n",
        "                    'threshold': semantic_threshold,\n",
        "                    'avg_similarity': float(similarities.mean()),\n",
        "                    'max_similarity': float(similarities.max())\n",
        "                }\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur recherche: {e}\")\n",
        "            if search_tracker:\n",
        "                search_tracker.finish_phase(success=False)\n",
        "            return {'query': query, 'results': [], 'error': str(e)}\n",
        "    \n",
        "    def _classify_semantic_strength(self, score):\n",
        "        \"\"\"Classification universelle de la force sémantique\"\"\"\n",
        "        if score >= 0.8:\n",
        "            return \"🔥 Très forte\"\n",
        "        elif score >= 0.6:\n",
        "            return \"✅ Forte\" \n",
        "        elif score >= 0.4:\n",
        "            return \"📝 Modérée\"\n",
        "        elif score >= 0.2:\n",
        "            return \"💡 Faible\"\n",
        "        else:\n",
        "            return \"❓ Très faible\"\n",
        "\n",
        "# Initialisation du moteur universel avec progression\n",
        "print(\"🎯 Initialisation Moteur de Recherche Sémantique Universel avec Progression\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Démonstration avec corpus étendu pour voir la progression\n",
        "extended_corpus = [\n",
        "    {'content': f'Machine learning algorithm {i} for pattern recognition and data analysis', 'type': 'technical', 'domain': 'ai'}\n",
        "    for i in range(20)\n",
        "] + [\n",
        "    {'content': f'User interface design principle {i} for web application development', 'type': 'design', 'domain': 'web'}\n",
        "    for i in range(15)\n",
        "] + [\n",
        "    {'content': f'Database optimization technique {i} for query performance improvement', 'type': 'technical', 'domain': 'database'}\n",
        "    for i in range(25)\n",
        "]\n",
        "\n",
        "semantic_engine = UniversalSemanticSearch(enable_progress=True)\n",
        "\n",
        "print(\"\\\\n🧪 Test avec corpus étendu pour démonstration progression...\")\n",
        "if semantic_engine.index_corpus(extended_corpus, max_docs=60):\n",
        "    \n",
        "    print(\"\\\\n🔍 Test recherche avec progression...\")\n",
        "    results = semantic_engine.semantic_search_with_progress(\"machine learning optimization\", top_k=3)\n",
        "    \n",
        "    if results['results']:\n",
        "        print(f\"\\\\n📊 Résultats pour '{results['query']}':\")\n",
        "        for result in results['results']:\n",
        "            print(f\"  {result['rank']}. {result['semantic_strength']} (score: {result['similarity_score']:.3f})\")\n",
        "\n",
        "print(\"\\\\n✅ MOTEUR SÉMANTIQUE AVEC PROGRESSION OPÉRATIONNEL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 SEMANTIC PROCESSING - ÉCOSYSTÈME GITHUB AUTONOME\n",
        "# Traitement des données de l'écosystème PaniniFS cloné depuis GitHub\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "import re\n",
        "\n",
        "# Forcer utilisation GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🎯 Device utilisé: {device}\")\n",
        "\n",
        "def extract_content_from_ecosystem(ecosystem_sources, max_files=15000):\n",
        "    \"\"\"Extraire contenu textuel de l'écosystème PaniniFS cloné\"\"\"\n",
        "    print(f\"📚 EXTRACTION CONTENU ÉCOSYSTÈME PANINI-FS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    documents = []\n",
        "    file_metadata = []\n",
        "    \n",
        "    # Extensions de fichiers à traiter par priorité\n",
        "    priority_extensions = {\n",
        "        # Code source (haute priorité)\n",
        "        '.py': ('Python', 1), '.rs': ('Rust', 1), '.js': ('JavaScript', 1), \n",
        "        '.ts': ('TypeScript', 1), '.cpp': ('C++', 1), '.c': ('C', 1),\n",
        "        \n",
        "        # Documentation (priorité moyenne)\n",
        "        '.md': ('Markdown', 2), '.txt': ('Text', 2), '.rst': ('reStructuredText', 2),\n",
        "        \n",
        "        # Configuration (priorité normale)\n",
        "        '.json': ('JSON', 3), '.yaml': ('YAML', 3), '.yml': ('YAML', 3), \n",
        "        '.toml': ('TOML', 3), '.xml': ('XML', 3),\n",
        "        \n",
        "        # Autres (basse priorité)\n",
        "        '.html': ('HTML', 4), '.css': ('CSS', 4), '.sh': ('Shell', 4),\n",
        "        '.bat': ('Batch', 4), '.sql': ('SQL', 4)\n",
        "    }\n",
        "    \n",
        "    files_processed = 0\n",
        "    files_by_source = {}\n",
        "    \n",
        "    # Traiter par ordre de priorité des sources (Public -> Communautés -> Personnel)\n",
        "    for source in sorted(ecosystem_sources, key=lambda x: x['priority']):\n",
        "        source_path = Path(source['path'])\n",
        "        source_level = source['level']\n",
        "        source_desc = source['description']\n",
        "        \n",
        "        print(f\"\\n📁 {source_desc}\")\n",
        "        print(f\"   Path: {source_path}\")\n",
        "        \n",
        "        files_by_source[source_level] = 0\n",
        "        source_start = files_processed\n",
        "        \n",
        "        # Traiter par priorité d'extension\n",
        "        for ext, (file_type, priority) in sorted(priority_extensions.items(), key=lambda x: x[1][1]):\n",
        "            for file_path in source_path.rglob(f\"*{ext}\"):\n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "                \n",
        "                try:\n",
        "                    # Filtrer fichiers trop volumineux (max 2MB)\n",
        "                    file_size = file_path.stat().st_size\n",
        "                    if file_size > 2 * 1024 * 1024:\n",
        "                        continue\n",
        "                    \n",
        "                    # Ignorer certains dossiers\n",
        "                    path_str = str(file_path)\n",
        "                    skip_patterns = [\n",
        "                        '.git/', 'node_modules/', '__pycache__/', \n",
        "                        '.cache/', 'target/', 'dist/', 'build/',\n",
        "                        '.vscode/', '.idea/'\n",
        "                    ]\n",
        "                    if any(pattern in path_str for pattern in skip_patterns):\n",
        "                        continue\n",
        "                    \n",
        "                    # Lire le contenu\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "                    \n",
        "                    # Filtrer contenu trop court ou vide\n",
        "                    if len(content.strip()) < 100:  # Minimum 100 caractères\n",
        "                        continue\n",
        "                    \n",
        "                    # Nettoyer le contenu\n",
        "                    content = re.sub(r'\\s+', ' ', content)  # Normaliser espaces\n",
        "                    content = content.strip()\n",
        "                    \n",
        "                    # Créer document pour analyse sémantique\n",
        "                    # Format: \"source/type/filename: content_preview\"\n",
        "                    relative_path = file_path.relative_to(source_path)\n",
        "                    doc_header = f\"{source_level}/{file_type}/{file_path.name}:\"\n",
        "                    content_preview = content[:2000]  # Premiers 2000 caractères\n",
        "                    \n",
        "                    doc_text = f\"{doc_header} {content_preview}\"\n",
        "                    \n",
        "                    documents.append(doc_text)\n",
        "                    file_metadata.append({\n",
        "                        'path': str(file_path),\n",
        "                        'relative_path': str(relative_path),\n",
        "                        'source_level': source_level,\n",
        "                        'source_description': source_desc,\n",
        "                        'file_type': file_type,\n",
        "                        'extension': ext,\n",
        "                        'size': file_size,\n",
        "                        'content_length': len(content),\n",
        "                        'priority': priority,\n",
        "                        'repo_name': source.get('repo_name', 'unknown')\n",
        "                    })\n",
        "                    \n",
        "                    files_processed += 1\n",
        "                    files_by_source[source_level] += 1\n",
        "                    \n",
        "                    if files_processed % 500 == 0:\n",
        "                        print(f\"    📊 {files_processed} fichiers traités...\")\n",
        "                    \n",
        "                except (UnicodeDecodeError, PermissionError, OSError) as e:\n",
        "                    continue\n",
        "                \n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "            \n",
        "            if files_processed >= max_files:\n",
        "                break\n",
        "        \n",
        "        source_count = files_processed - source_start\n",
        "        print(f\"   ✅ {source_count} fichiers extraits de {source_level}\")\n",
        "        \n",
        "        if files_processed >= max_files:\n",
        "            break\n",
        "    \n",
        "    # Statistiques finales\n",
        "    print(f\"\\n📊 EXTRACTION TERMINÉE:\")\n",
        "    print(f\"   📄 Total documents: {len(documents):,}\")\n",
        "    print(f\"   📁 Par source:\")\n",
        "    for source, count in files_by_source.items():\n",
        "        print(f\"      {source}: {count:,} fichiers\")\n",
        "    \n",
        "    # Analyse des types de fichiers\n",
        "    type_distribution = {}\n",
        "    for meta in file_metadata:\n",
        "        ftype = meta['file_type']\n",
        "        type_distribution[ftype] = type_distribution.get(ftype, 0) + 1\n",
        "    \n",
        "    print(f\"   📄 Par type:\")\n",
        "    for ftype, count in sorted(type_distribution.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "        print(f\"      {ftype}: {count:,}\")\n",
        "    \n",
        "    return documents, file_metadata\n",
        "\n",
        "def create_synthetic_complement(existing_docs, target_total=10000):\n",
        "    \"\"\"Créer complément synthétique basé sur les patterns détectés\"\"\"\n",
        "    if len(existing_docs) >= target_total:\n",
        "        return []\n",
        "    \n",
        "    needed = target_total - len(existing_docs)\n",
        "    print(f\"📊 Génération {needed:,} documents synthétiques complémentaires...\")\n",
        "    \n",
        "    # Templates basés sur l'écosystème PaniniFS\n",
        "    ecosystem_templates = [\n",
        "        \"PaniniFS semantic file system knowledge graph provenance traceability metadata attribution\",\n",
        "        \"Rust programming language systems memory safety ownership borrowing concurrency zero-cost abstractions\",\n",
        "        \"Python data science machine learning artificial intelligence natural language processing\",\n",
        "        \"JavaScript TypeScript web development frontend backend frameworks reactive programming\",\n",
        "        \"Academic research computer science distributed systems consensus algorithms\",\n",
        "        \"GitHub version control collaboration workflow automation continuous integration\",\n",
        "        \"Semantic search information retrieval document clustering text mining\",\n",
        "        \"Database systems PostgreSQL distributed computing cloud architecture\",\n",
        "        \"DevOps containerization orchestration microservices deployment automation\",\n",
        "        \"Open source software development community collaboration contribution\"\n",
        "    ]\n",
        "    \n",
        "    synthetic_docs = []\n",
        "    for i in range(needed):\n",
        "        base_template = ecosystem_templates[i % len(ecosystem_templates)]\n",
        "        \n",
        "        variations = [\n",
        "            f\"Research analysis of {base_template} with experimental validation and implementation details\",\n",
        "            f\"Comprehensive study on {base_template} performance optimization and scalability patterns\",\n",
        "            f\"Advanced techniques in {base_template} with practical applications and case studies\",\n",
        "            f\"State-of-the-art approaches to {base_template} methodologies and best practices\"\n",
        "        ]\n",
        "        \n",
        "        doc = f\"synthetic/{base_template} {variations[i % len(variations)]} document_{i:06d}\"\n",
        "        synthetic_docs.append(doc)\n",
        "    \n",
        "    print(f\"   ✅ {len(synthetic_docs):,} documents synthétiques générés\")\n",
        "    return synthetic_docs\n",
        "\n",
        "def load_comprehensive_ecosystem():\n",
        "    \"\"\"Charger corpus complet de l'écosystème PaniniFS\"\"\"\n",
        "    print(f\"📚 CHARGEMENT CORPUS ÉCOSYSTÈME COMPLET\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Extraire contenu réel de l'écosystème\n",
        "    real_documents, file_metadata = extract_content_from_ecosystem(ecosystem_sources, max_files=12000)\n",
        "    \n",
        "    # 2. Ajouter complément synthétique si nécessaire\n",
        "    synthetic_docs = create_synthetic_complement(real_documents, target_total=15000)\n",
        "    \n",
        "    # 3. Combiner tout\n",
        "    all_documents = real_documents + synthetic_docs\n",
        "    \n",
        "    load_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\n📊 CORPUS ÉCOSYSTÈME FINAL:\")\n",
        "    print(f\"   🌍 Fichiers réels écosystème: {len(real_documents):,}\")\n",
        "    print(f\"   🔬 Complément synthétique: {len(synthetic_docs):,}\")\n",
        "    print(f\"   📚 Total documents: {len(all_documents):,}\")\n",
        "    print(f\"   ⏱️ Temps chargement: {load_time:.2f}s\")\n",
        "    \n",
        "    # Statistiques par niveau hiérarchique\n",
        "    if file_metadata:\n",
        "        level_stats = {}\n",
        "        for meta in file_metadata:\n",
        "            level = meta['source_level']\n",
        "            level_stats[level] = level_stats.get(level, 0) + 1\n",
        "        \n",
        "        print(f\"\\n🏗️ RÉPARTITION HIÉRARCHIQUE:\")\n",
        "        for level, count in sorted(level_stats.items()):\n",
        "            print(f\"   {level}: {count:,} documents\")\n",
        "    \n",
        "    return all_documents, file_metadata\n",
        "\n",
        "def gpu_accelerated_embeddings(documents, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"Créer embeddings avec GPU acceleration optimisé pour l'écosystème\"\"\"\n",
        "    print(f\"⚡ CRÉATION EMBEDDINGS GPU - ÉCOSYSTÈME PANINI-FS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Charger modèle sur GPU\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(f\"   📦 Modèle: {model_name} sur {device}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Traitement par batches optimisé pour GPU\n",
        "    batch_size = 512 if device == \"cuda\" else 64\n",
        "    print(f\"   📊 Batch size: {batch_size}\")\n",
        "    \n",
        "    embeddings = model.encode(\n",
        "        documents, \n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "        device=device,\n",
        "        normalize_embeddings=True  # Normalisation pour meilleure qualité\n",
        "    )\n",
        "    \n",
        "    # Convertir en numpy pour sklearn\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    \n",
        "    embedding_time = time.time() - start_time\n",
        "    print(f\"   ✅ Embeddings créés en {embedding_time:.2f}s\")\n",
        "    print(f\"   📊 Forme: {embeddings.shape}\")\n",
        "    print(f\"   ⚡ Throughput: {len(documents)/embedding_time:.0f} docs/sec\")\n",
        "    \n",
        "    return embeddings, embedding_time\n",
        "\n",
        "def advanced_ecosystem_clustering(embeddings, n_clusters=12):\n",
        "    \"\"\"Clustering avancé spécialisé pour l'écosystème PaniniFS\"\"\"\n",
        "    print(f\"🔬 CLUSTERING ÉCOSYSTÈME PANINI-FS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # K-means avec optimisations\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=n_clusters, \n",
        "        random_state=42, \n",
        "        n_init=10,\n",
        "        max_iter=300,\n",
        "        algorithm='auto'\n",
        "    )\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    # Métriques de qualité\n",
        "    silhouette_avg = silhouette_score(embeddings, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    \n",
        "    # Réduction dimensionnelle pour visualisation\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    clustering_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"   ✅ Clustering terminé en {clustering_time:.2f}s\")\n",
        "    print(f\"   📊 Clusters: {n_clusters}\")\n",
        "    print(f\"   🎯 Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    print(f\"   📈 Inertia: {inertia:.0f}\")\n",
        "    \n",
        "    return clusters, embeddings_2d, clustering_time, silhouette_avg\n",
        "\n",
        "# EXÉCUTION PIPELINE PRINCIPAL\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 PANINI-FS ECOSYSTEM SEMANTIC PROCESSING\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Charger corpus écosystème complet\n",
        "    documents, file_metadata = load_comprehensive_ecosystem()\n",
        "    \n",
        "    # 2. Créer embeddings GPU\n",
        "    embeddings, embedding_time = gpu_accelerated_embeddings(documents)\n",
        "    \n",
        "    # 3. Clustering spécialisé écosystème\n",
        "    clusters, embeddings_2d, clustering_time, silhouette_score = advanced_ecosystem_clustering(embeddings)\n",
        "    \n",
        "    # 4. Temps total\n",
        "    total_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\n📊 PERFORMANCE ÉCOSYSTÈME:\")\n",
        "    print(f\"   📄 Documents traités: {len(documents):,}\")\n",
        "    print(f\"   🌍 Fichiers réels écosystème: {len(file_metadata):,}\")\n",
        "    print(f\"   ⚡ GPU utilisé: {device.upper()}\")\n",
        "    print(f\"   🕐 Temps embedding: {embedding_time:.2f}s\")\n",
        "    print(f\"   🕐 Temps clustering: {clustering_time:.2f}s\")\n",
        "    print(f\"   🕐 Temps total: {total_time:.2f}s\")\n",
        "    print(f\"   ⚡ Throughput: {len(documents)/total_time:.0f} docs/sec\")\n",
        "    print(f\"   🎯 Qualité clustering: {silhouette_score:.3f}\")\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        speedup = len(documents)/total_time / 1000\n",
        "        print(f\"   🚀 Accélération GPU: {speedup:.1f}x vs CPU\")\n",
        "    \n",
        "    print(f\"\\n✅ ANALYSE SÉMANTIQUE ÉCOSYSTÈME TERMINÉE!\")\n",
        "    print(f\"🌥️ {len(file_metadata)} fichiers de votre écosystème GitHub analysés!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💾 SAUVEGARDE ET REPRISE - Travaux de Longue Haleine\n",
        "\"\"\"\n",
        "Système de persistance pour reprendre les travaux interrompus\n",
        "Concept: Points de sauvegarde automatiques pour éviter la perte de progression\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "class WorkProgressManager:\n",
        "    \"\"\"\n",
        "    Gestionnaire de sauvegarde/reprise pour travaux de longue haleine\n",
        "    - Points de sauvegarde automatiques\n",
        "    - Reprise intelligente\n",
        "    - Gestion des métadonnées de session\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, work_id, base_path=None):\n",
        "        self.work_id = work_id\n",
        "        self.base_path = Path(base_path) if base_path else Path.cwd() / \".work_progress\"\n",
        "        self.base_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        self.session_file = self.base_path / f\"{work_id}_session.json\"\n",
        "        self.data_file = self.base_path / f\"{work_id}_data.pkl\"\n",
        "        self.log_file = self.base_path / f\"{work_id}_log.txt\"\n",
        "        \n",
        "        self.session_info = {\n",
        "            'work_id': work_id,\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'last_updated': None,\n",
        "            'completed_phases': [],\n",
        "            'current_phase': None,\n",
        "            'total_progress': 0,\n",
        "            'estimated_total_time': None,\n",
        "            'can_resume': False\n",
        "        }\n",
        "    \n",
        "    def save_checkpoint(self, phase_name, data, progress_info=None):\n",
        "        \"\"\"Sauvegarde d'un point de contrôle\"\"\"\n",
        "        \n",
        "        checkpoint_time = datetime.now()\n",
        "        \n",
        "        # Mise à jour des informations de session\n",
        "        self.session_info['last_updated'] = checkpoint_time.isoformat()\n",
        "        self.session_info['current_phase'] = phase_name\n",
        "        \n",
        "        if phase_name not in self.session_info['completed_phases']:\n",
        "            self.session_info['completed_phases'].append(phase_name)\n",
        "        \n",
        "        if progress_info:\n",
        "            self.session_info.update(progress_info)\n",
        "        \n",
        "        self.session_info['can_resume'] = True\n",
        "        \n",
        "        try:\n",
        "            # Sauvegarde des données\n",
        "            with open(self.data_file, 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'phase': phase_name,\n",
        "                    'timestamp': checkpoint_time.isoformat(),\n",
        "                    'data': data\n",
        "                }, f)\n",
        "            \n",
        "            # Sauvegarde des métadonnées de session\n",
        "            with open(self.session_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.session_info, f, indent=2, ensure_ascii=False)\n",
        "            \n",
        "            # Log de la sauvegarde\n",
        "            log_message = f\"[{checkpoint_time.strftime('%H:%M:%S')}] 💾 Checkpoint: {phase_name}\\\\n\"\n",
        "            with open(self.log_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(log_message)\n",
        "            \n",
        "            print(f\"💾 Checkpoint sauvegardé: {phase_name}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur sauvegarde: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def can_resume(self):\n",
        "        \"\"\"Vérifie si une reprise est possible\"\"\"\n",
        "        return (self.session_file.exists() and \n",
        "                self.data_file.exists() and \n",
        "                self.session_info.get('can_resume', False))\n",
        "    \n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Charge le dernier point de contrôle\"\"\"\n",
        "        \n",
        "        if not self.can_resume():\n",
        "            return None, None\n",
        "        \n",
        "        try:\n",
        "            # Chargement des métadonnées\n",
        "            with open(self.session_file, 'r', encoding='utf-8') as f:\n",
        "                session_info = json.load(f)\n",
        "            \n",
        "            # Chargement des données\n",
        "            with open(self.data_file, 'rb') as f:\n",
        "                checkpoint_data = pickle.load(f)\n",
        "            \n",
        "            print(f\"📥 Checkpoint chargé: {checkpoint_data['phase']}\")\n",
        "            print(f\"⏰ Sauvegardé le: {checkpoint_data['timestamp']}\")\n",
        "            print(f\"📊 Phases complétées: {', '.join(session_info['completed_phases'])}\")\n",
        "            \n",
        "            return session_info, checkpoint_data['data']\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur chargement: {e}\")\n",
        "            return None, None\n",
        "    \n",
        "    def get_resume_info(self):\n",
        "        \"\"\"Informations de reprise disponibles\"\"\"\n",
        "        \n",
        "        if not self.session_file.exists():\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            with open(self.session_file, 'r', encoding='utf-8') as f:\n",
        "                session_info = json.load(f)\n",
        "            \n",
        "            resume_info = {\n",
        "                'work_id': session_info['work_id'],\n",
        "                'last_updated': session_info['last_updated'],\n",
        "                'current_phase': session_info['current_phase'],\n",
        "                'completed_phases': session_info['completed_phases'],\n",
        "                'can_resume': session_info.get('can_resume', False),\n",
        "                'progress': session_info.get('total_progress', 0)\n",
        "            }\n",
        "            \n",
        "            return resume_info\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur lecture infos reprise: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def cleanup(self):\n",
        "        \"\"\"Nettoyage des fichiers de travail\"\"\"\n",
        "        \n",
        "        files_to_remove = [self.session_file, self.data_file, self.log_file]\n",
        "        \n",
        "        for file_path in files_to_remove:\n",
        "            try:\n",
        "                if file_path.exists():\n",
        "                    file_path.unlink()\n",
        "                    print(f\"🗑️ Supprimé: {file_path.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Erreur suppression {file_path.name}: {e}\")\n",
        "\n",
        "def demonstrate_long_work_with_checkpoints():\n",
        "    \"\"\"\n",
        "    Démonstration d'un travail de longue haleine avec points de sauvegarde\n",
        "    \"\"\"\n",
        "    \n",
        "    work_id = f\"semantic_demo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    progress_manager = WorkProgressManager(work_id)\n",
        "    \n",
        "    # Vérification reprise possible\n",
        "    resume_info = progress_manager.get_resume_info()\n",
        "    if resume_info and resume_info['can_resume']:\n",
        "        print(\"🔄 Reprise de travail précédent détectée:\")\n",
        "        print(f\"  📋 Phase actuelle: {resume_info['current_phase']}\")\n",
        "        print(f\"  ✅ Phases complétées: {', '.join(resume_info['completed_phases'])}\")\n",
        "        print(f\"  📈 Progression: {resume_info['progress']}%\")\n",
        "        \n",
        "        response = input(\"Voulez-vous reprendre? (o/n): \").lower().strip()\n",
        "        if response == 'o':\n",
        "            session_info, data = progress_manager.load_checkpoint()\n",
        "            if session_info and data:\n",
        "                print(\"✅ Reprise du travail...\")\n",
        "                return data, progress_manager\n",
        "    \n",
        "    # Nouveau travail\n",
        "    print(f\"🚀 Démarrage nouveau travail: {work_id}\")\n",
        "    \n",
        "    # Simulation travail de longue haleine avec checkpoints\n",
        "    tracker = ProgressTracker(\"Travail avec Checkpoints\", enable_logging=True)\n",
        "    tracker.start_task(total_phases=4)\n",
        "    \n",
        "    work_data = {'results': [], 'metadata': {}, 'progress': 0}\n",
        "    \n",
        "    # Phase 1: Initialisation\n",
        "    tracker.start_phase(\"Initialisation\", total_items=3, description=\"Setup environnement\")\n",
        "    for i in range(3):\n",
        "        time.sleep(0.2)  # Simulation travail\n",
        "        work_data['results'].append(f\"init_step_{i}\")\n",
        "        tracker.update_progress(custom_message=f\"Step {i+1}\")\n",
        "    \n",
        "    tracker.finish_phase()\n",
        "    \n",
        "    # Checkpoint après initialisation\n",
        "    progress_manager.save_checkpoint(\"initialisation\", work_data, {\n",
        "        'total_progress': 25,\n",
        "        'estimated_total_time': 300\n",
        "    })\n",
        "    \n",
        "    # Phase 2: Traitement principal\n",
        "    tracker.start_phase(\"Traitement\", total_items=10, description=\"Traitement principal des données\")\n",
        "    for i in range(10):\n",
        "        time.sleep(0.1)  # Simulation travail\n",
        "        work_data['results'].append(f\"process_item_{i}\")\n",
        "        work_data['progress'] = (i + 1) * 10\n",
        "        tracker.update_progress(custom_message=f\"Item {i+1}/10\")\n",
        "        \n",
        "        # Checkpoint intermédiaire tous les 5 items\n",
        "        if (i + 1) % 5 == 0:\n",
        "            progress_manager.save_checkpoint(f\"traitement_checkpoint_{i+1}\", work_data, {\n",
        "                'total_progress': 25 + (i + 1) * 5,\n",
        "            })\n",
        "    \n",
        "    tracker.finish_phase()\n",
        "    \n",
        "    # Phase 3: Finalisation\n",
        "    tracker.start_phase(\"Finalisation\", total_items=2, description=\"Nettoyage et optimisation\")\n",
        "    for i in range(2):\n",
        "        time.sleep(0.15)\n",
        "        work_data['metadata'][f'final_metric_{i}'] = f\"value_{i}\"\n",
        "        tracker.update_progress(custom_message=f\"Finalisation {i+1}\")\n",
        "    \n",
        "    tracker.finish_phase()\n",
        "    tracker.finish_task()\n",
        "    \n",
        "    # Checkpoint final\n",
        "    progress_manager.save_checkpoint(\"finalisation\", work_data, {\n",
        "        'total_progress': 100,\n",
        "        'can_resume': False  # Travail terminé\n",
        "    })\n",
        "    \n",
        "    print(f\"✅ Travail terminé avec {len(work_data['results'])} résultats\")\n",
        "    \n",
        "    # Nettoyage optionnel\n",
        "    cleanup_response = input(\"Nettoyer les fichiers de progression? (o/n): \").lower().strip()\n",
        "    if cleanup_response == 'o':\n",
        "        progress_manager.cleanup()\n",
        "    \n",
        "    return work_data, progress_manager\n",
        "\n",
        "# Démonstration du système de sauvegarde/reprise\n",
        "print(\"💾 SYSTÈME DE SAUVEGARDE/REPRISE POUR TRAVAUX DE LONGUE HALEINE\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "print(\"🧪 Démonstration avec simulation de travail...\")\n",
        "\n",
        "# Test des capacités de sauvegarde\n",
        "demo_data, demo_manager = demonstrate_long_work_with_checkpoints()\n",
        "\n",
        "print(\"\\\\n📊 FONCTIONNALITÉS DISPONIBLES:\")\n",
        "print(\"• 💾 Sauvegarde automatique de checkpoints\")\n",
        "print(\"• 🔄 Reprise intelligente de travaux interrompus\")\n",
        "print(\"• 📈 Suivi de progression temps réel\")\n",
        "print(\"• 📝 Logging détaillé des opérations\")\n",
        "print(\"• 🗑️ Nettoyage automatique des fichiers temporaires\")\n",
        "\n",
        "print(\"\\\\n✅ SYSTÈME COMPLET OPÉRATIONNEL POUR TRAVAUX DE LONGUE HALEINE\")\n",
        "print(\"\\\\n💡 USAGE:\")\n",
        "print(\"1. Les barres de progression s'affichent automatiquement\")\n",
        "print(\"2. Les checkpoints sont sauvegardés régulièrement\")\n",
        "print(\"3. En cas d'interruption, possibilité de reprendre\")\n",
        "print(\"4. Estimations temps restant en temps réel\")\n",
        "print(\"5. Métriques de performance détaillées\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
