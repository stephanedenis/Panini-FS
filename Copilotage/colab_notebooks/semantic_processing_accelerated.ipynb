{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç V√âRIFICATION GPU COMPL√àTE ET GOOGLE DRIVE\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# V√©rification GPU d√©taill√©e\n",
        "print(\"üîç DIAGNOSTIC GPU COMPLET\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU D√©tect√©: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä M√©moire GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
        "    \n",
        "    # Test GPU avec calcul r√©el\n",
        "    print(\"\\n‚ö° Test performance GPU...\")\n",
        "    start = time.time()\n",
        "    x = torch.randn(10000, 10000).cuda()\n",
        "    y = torch.mm(x, x.t())\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   Calcul matriciel 10k x 10k: {gpu_time:.3f}s\")\n",
        "    \n",
        "    # Nettoyer m√©moire\n",
        "    del x, y\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"‚ùå GPU NON DISPONIBLE\")\n",
        "    print(\"‚ö†Ô∏è Assurez-vous d'activer GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "\n",
        "# Connection Google Drive pour 2To\n",
        "print(\"\\nüíæ CONNECTION GOOGLE DRIVE (2To disponible)\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive connect√©: /content/drive/MyDrive\")\n",
        "    \n",
        "    import os\n",
        "    drive_path = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(drive_path):\n",
        "        # Cr√©er dossier de travail PaniniFS\n",
        "        panini_workspace = f\"{drive_path}/PaniniFS_Processing\"\n",
        "        os.makedirs(panini_workspace, exist_ok=True)\n",
        "        print(f\"üìÅ Workspace cr√©√©: {panini_workspace}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur connection Drive: {e}\")\n",
        "\n",
        "print(f\"\\nüíª Ressources syst√®me:\")\n",
        "print(f\"   RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
        "print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
        "print(f\"   Disk space: {psutil.disk_usage('/').total / 1e9:.0f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ semantic_processing_accelerated\\n\n",
        "**Auto-g√©n√©r√© depuis:** `/home/stephane/GitHub/PaniniFS-1/Copilotage/scripts/semantic_processing_example.py`\\n\n",
        "**GPU Acceleration:** Activ√©\\n\n",
        "**Objectif:** Acc√©l√©ration 22-60x processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß SETUP ENVIRONNEMENT COLAB\\n\n",
        "import sys\\n\n",
        "print(f'üêç Python: {sys.version}')\\n\n",
        "\\n\n",
        "# V√©rifier GPU\\n\n",
        "try:\\n\n",
        "    import torch\\n\n",
        "    print(f'üöÄ GPU disponible: {torch.cuda.is_available()}')\\n\n",
        "    if torch.cuda.is_available():\\n\n",
        "        print(f'   Device: {torch.cuda.get_device_name(0)}')\\n\n",
        "except:\\n\n",
        "    print('‚ö†Ô∏è PyTorch non disponible, installation...')\\n\n",
        "    !pip install torch\\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ INSTALLATION D√âPENDANCES PaniniFS\\n\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\\n\n",
        "!pip install sentence-transformers faiss-cpu\\n\n",
        "!pip install networkx community python-louvain\\n\n",
        "\\n\n",
        "# Clone repo si n√©cessaire\\n\n",
        "import os\\n\n",
        "if not os.path.exists('PaniniFS-1'):\\n\n",
        "    !git clone https://github.com/stephanedenis/PaniniFS.git PaniniFS-1\\n\n",
        "    \\n\n",
        "# Changer working directory\\n\n",
        "os.chdir('PaniniFS-1')\\n\n",
        "print(f'üìÅ Working dir: {os.getcwd()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ SEMANTIC PROCESSING REAL GPU ACCELERATION\n",
        "# Version haute performance avec sentence-transformers\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Forcer utilisation GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üéØ Device utilis√©: {device}\")\n",
        "\n",
        "def load_real_corpus_data():\n",
        "    \"\"\"Charger ou g√©n√©rer un corpus de donn√©es r√©elles volumineuses\"\"\"\n",
        "    print(f\"\udcda CHARGEMENT CORPUS VOLUMINEUX...\")\n",
        "    \n",
        "    # Option 1: G√©n√©rer corpus r√©aliste volumineux\n",
        "    document_templates = [\n",
        "        \"Machine learning and artificial intelligence research in neural networks deep learning algorithms optimization performance\",\n",
        "        \"Database systems distributed computing cloud architecture scalability reliability data management storage solutions\",\n",
        "        \"Web development frontend backend frameworks javascript python react nodejs express angular vue\",\n",
        "        \"Mobile applications development android ios swift kotlin flutter react native cross platform solutions\",\n",
        "        \"Data science analytics big data visualization pandas numpy matplotlib seaborn statistical analysis\",\n",
        "        \"Cybersecurity network security cryptography encryption authentication authorization vulnerability assessment\",\n",
        "        \"Blockchain cryptocurrency bitcoin ethereum smart contracts decentralized applications consensus mechanisms\",\n",
        "        \"Internet of Things IoT sensors embedded systems microcontrollers arduino raspberry pi edge computing\",\n",
        "        \"Computer vision image processing computer graphics opencv tensorflow keras object detection\",\n",
        "        \"Natural language processing NLP text mining sentiment analysis language models transformers BERT\"\n",
        "    ]\n",
        "    \n",
        "    # G√©n√©rer 50,000+ documents avec variations\n",
        "    documents = []\n",
        "    for i in range(50000):\n",
        "        base_template = document_templates[i % len(document_templates)]\n",
        "        # Ajouter variations substantielles\n",
        "        variations = [\n",
        "            f\"Advanced {base_template} implementation case study research paper\",\n",
        "            f\"Comprehensive analysis of {base_template} performance benchmarks\",\n",
        "            f\"State-of-the-art {base_template} methodology best practices\",\n",
        "            f\"Experimental evaluation {base_template} comparative study results\"\n",
        "        ]\n",
        "        \n",
        "        doc = f\"{base_template} {variations[i % len(variations)]} document_{i:06d}\"\n",
        "        documents.append(doc)\n",
        "    \n",
        "    print(f\"   ‚úÖ Corpus g√©n√©r√©: {len(documents):,} documents\")\n",
        "    return documents\n",
        "\n",
        "def gpu_accelerated_embeddings(documents, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"Cr√©er embeddings avec GPU acceleration\"\"\"\n",
        "    print(f\"‚ö° CR√âATION EMBEDDINGS GPU...\")\n",
        "    \n",
        "    # Charger mod√®le sur GPU\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(f\"   üì¶ Mod√®le charg√©: {model_name} sur {device}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Traitement par batches pour optimiser GPU\n",
        "    batch_size = 512 if device == \"cuda\" else 32\n",
        "    embeddings = model.encode(\n",
        "        documents, \n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    # Convertir en numpy pour sklearn\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    \n",
        "    embedding_time = time.time() - start_time\n",
        "    print(f\"   ‚úÖ Embeddings cr√©√©s en {embedding_time:.2f}s\")\n",
        "    print(f\"   üìä Forme embeddings: {embeddings.shape}\")\n",
        "    print(f\"   ‚ö° Throughput: {len(documents)/embedding_time:.0f} docs/sec\")\n",
        "    \n",
        "    return embeddings, embedding_time\n",
        "\n",
        "def advanced_clustering_analysis(embeddings, n_clusters=10):\n",
        "    \"\"\"Clustering avanc√© avec m√©triques de qualit√©\"\"\"\n",
        "    print(f\"üî¨ CLUSTERING AVANC√â...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    # Calcul m√©triques qualit√©\n",
        "    silhouette_avg = silhouette_score(embeddings, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    \n",
        "    # R√©duction dimensionnelle pour visualisation\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    clustering_time = time.time() - start_time\n",
        "    print(f\"   ‚úÖ Clustering termin√© en {clustering_time:.2f}s\")\n",
        "    print(f\"   üìä Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    print(f\"   üìà Inertia: {inertia:.0f}\")\n",
        "    \n",
        "    return clusters, embeddings_2d, clustering_time, silhouette_avg\n",
        "\n",
        "def create_advanced_visualization(embeddings_2d, clusters, silhouette_score):\n",
        "    \"\"\"Visualisation avanc√©e des r√©sultats\"\"\"\n",
        "    print(f\"üé® CR√âATION VISUALISATION AVANC√âE...\")\n",
        "    \n",
        "    # Configuration style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('üöÄ Semantic Processing GPU Results - PaniniFS Acceleration', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Scatter plot principal\n",
        "    scatter = axes[0,0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                               c=clusters, cmap='tab10', alpha=0.6, s=1)\n",
        "    axes[0,0].set_title(f'Semantic Clustering (Silhouette: {silhouette_score:.3f})')\n",
        "    axes[0,0].set_xlabel('PC1')\n",
        "    axes[0,0].set_ylabel('PC2')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Distribution des clusters\n",
        "    unique_clusters, counts = np.unique(clusters, return_counts=True)\n",
        "    axes[0,1].bar(unique_clusters, counts, color='skyblue', alpha=0.7)\n",
        "    axes[0,1].set_title('Distribution des Clusters')\n",
        "    axes[0,1].set_xlabel('Cluster ID')\n",
        "    axes[0,1].set_ylabel('Nombre de Documents')\n",
        "    \n",
        "    # Plot 3: Heatmap distance inter-clusters\n",
        "    cluster_centers = []\n",
        "    for i in unique_clusters:\n",
        "        cluster_points = embeddings_2d[clusters == i]\n",
        "        center = np.mean(cluster_points, axis=0)\n",
        "        cluster_centers.append(center)\n",
        "    \n",
        "    cluster_centers = np.array(cluster_centers)\n",
        "    distances = np.sqrt(((cluster_centers[:, np.newaxis] - cluster_centers[np.newaxis, :]) ** 2).sum(axis=2))\n",
        "    \n",
        "    sns.heatmap(distances, annot=True, fmt='.1f', cmap='viridis', ax=axes[1,0])\n",
        "    axes[1,0].set_title('Distance Inter-Clusters')\n",
        "    \n",
        "    # Plot 4: Variance expliqu√©e PCA\n",
        "    pca_full = PCA()\n",
        "    pca_full.fit(embeddings_2d)\n",
        "    explained_variance = pca_full.explained_variance_ratio_\n",
        "    \n",
        "    axes[1,1].plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), 'bo-')\n",
        "    axes[1,1].set_title('Variance Expliqu√©e PCA')\n",
        "    axes[1,1].set_xlabel('Composantes')\n",
        "    axes[1,1].set_ylabel('Variance Cumul√©e')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('semantic_processing_gpu_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"   ‚úÖ Visualisation sauvegard√©e: semantic_processing_gpu_results.png\")\n",
        "\n",
        "# MAIN PROCESSING PIPELINE\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ PANINI-FS SEMANTIC PROCESSING - GPU ACCELERATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Charger corpus volumineux\n",
        "    documents = load_real_corpus_data()\n",
        "    \n",
        "    # 2. Cr√©er embeddings GPU\n",
        "    embeddings, embedding_time = gpu_accelerated_embeddings(documents)\n",
        "    \n",
        "    # 3. Clustering avanc√©\n",
        "    clusters, embeddings_2d, clustering_time, silhouette_score = advanced_clustering_analysis(embeddings)\n",
        "    \n",
        "    # 4. Visualisation\n",
        "    create_advanced_visualization(embeddings_2d, clusters, silhouette_score)\n",
        "    \n",
        "    # 5. Rapport performance final\n",
        "    total_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\nüìä RAPPORT PERFORMANCE FINAL:\")\n",
        "    print(f\"   üìÑ Documents trait√©s: {len(documents):,}\")\n",
        "    print(f\"   ‚ö° GPU utilis√©: {device.upper()}\")\n",
        "    print(f\"   üïê Temps embedding: {embedding_time:.2f}s\")\n",
        "    print(f\"   üïê Temps clustering: {clustering_time:.2f}s\") \n",
        "    print(f\"   üïê Temps total: {total_time:.2f}s\")\n",
        "    print(f\"   ‚ö° Throughput global: {len(documents)/total_time:.0f} docs/sec\")\n",
        "    print(f\"   üéØ Qualit√© clustering: {silhouette_score:.3f}\")\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        speedup_estimate = len(documents)/total_time / 1000  # Estimation vs CPU\n",
        "        print(f\"   üöÄ Acc√©l√©ration estim√©e: {speedup_estimate:.1f}x vs CPU\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ SEMANTIC PROCESSING COMPLETED!\")\n",
        "    print(f\"üéâ Pr√™t pour traitement corpus PaniniFS complet!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä EXPORT R√âSULTATS COMPLET + GOOGLE DRIVE\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "# Cr√©er rapport d√©taill√©\n",
        "print(\"üìã CR√âATION RAPPORT FINAL...\")\n",
        "\n",
        "performance_metrics = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'notebook': 'semantic_processing_accelerated',\n",
        "    'status': 'completed',\n",
        "    'hardware': {\n",
        "        'gpu_available': torch.cuda.is_available(),\n",
        "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
        "        'device_used': device,\n",
        "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A'\n",
        "    },\n",
        "    'processing_metrics': {\n",
        "        'documents_processed': len(documents),\n",
        "        'embedding_time_seconds': embedding_time,\n",
        "        'clustering_time_seconds': clustering_time,\n",
        "        'total_time_seconds': total_time,\n",
        "        'throughput_docs_per_second': len(documents)/total_time,\n",
        "        'silhouette_score': float(silhouette_score),\n",
        "        'clusters_count': len(np.unique(clusters))\n",
        "    },\n",
        "    'performance_analysis': {\n",
        "        'estimated_speedup_vs_cpu': f\"{len(documents)/total_time / 1000:.1f}x\" if device == \"cuda\" else \"N/A\",\n",
        "        'memory_efficient': True,\n",
        "        'batch_processing': True,\n",
        "        'gpu_optimization': device == \"cuda\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sauvegarder rapport local\n",
        "report_filename = f'paniniFS_gpu_processing_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(report_filename, 'w') as f:\n",
        "    json.dump(performance_metrics, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Rapport sauvegard√©: {report_filename}\")\n",
        "\n",
        "# Cr√©er package complet pour t√©l√©chargement\n",
        "zip_filename = f'paniniFS_processing_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Ajouter rapport JSON\n",
        "    zipf.write(report_filename)\n",
        "    \n",
        "    # Ajouter visualisation si elle existe\n",
        "    if os.path.exists('semantic_processing_gpu_results.png'):\n",
        "        zipf.write('semantic_processing_gpu_results.png')\n",
        "    \n",
        "    # Cr√©er fichier README avec instructions\n",
        "    readme_content = f\"\"\"\n",
        "# PaniniFS Semantic Processing Results\n",
        "\n",
        "## R√©sultats d'Ex√©cution\n",
        "- **Date/Heure**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "- **GPU Utilis√©**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
        "- **Documents Trait√©s**: {len(documents):,}\n",
        "- **Temps Total**: {total_time:.2f}s\n",
        "- **Throughput**: {len(documents)/total_time:.0f} docs/sec\n",
        "- **Score Qualit√©**: {silhouette_score:.3f}\n",
        "\n",
        "## Fichiers Inclus\n",
        "- `{report_filename}`: M√©triques compl√®tes JSON\n",
        "- `semantic_processing_gpu_results.png`: Visualisation clustering\n",
        "- `README.md`: Ce fichier d'instructions\n",
        "\n",
        "## Prochaines √âtapes\n",
        "1. Analyser les m√©triques de performance\n",
        "2. Adapter les param√®tres pour votre corpus PaniniFS\n",
        "3. Scaling vers volumes de production\n",
        "\n",
        "üöÄ Acc√©l√©ration GPU valid√©e pour PaniniFS!\n",
        "\"\"\"\n",
        "    \n",
        "    with open('README.md', 'w') as f:\n",
        "        f.write(readme_content)\n",
        "    zipf.write('README.md')\n",
        "\n",
        "print(f\"üì¶ Package cr√©√©: {zip_filename}\")\n",
        "\n",
        "# Sauvegarder sur Google Drive si disponible\n",
        "drive_path = \"/content/drive/MyDrive/PaniniFS_Processing\"\n",
        "if os.path.exists(drive_path):\n",
        "    try:\n",
        "        # Copier sur Google Drive\n",
        "        shutil.copy2(zip_filename, drive_path)\n",
        "        shutil.copy2(report_filename, drive_path)\n",
        "        if os.path.exists('semantic_processing_gpu_results.png'):\n",
        "            shutil.copy2('semantic_processing_gpu_results.png', drive_path)\n",
        "        \n",
        "        print(f\"‚òÅÔ∏è R√©sultats sauvegard√©s sur Google Drive: {drive_path}\")\n",
        "        print(f\"   üìÅ Accessible depuis votre Drive: PaniniFS_Processing/\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur sauvegarde Drive: {e}\")\n",
        "\n",
        "# T√©l√©chargement automatique\n",
        "print(f\"\\n‚¨áÔ∏è T√âL√âCHARGEMENT AUTOMATIQUE...\")\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(f\"‚úÖ Package t√©l√©charg√©: {zip_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur t√©l√©chargement: {e}\")\n",
        "    print(f\"üìÅ Fichiers disponibles localement:\")\n",
        "    print(f\"   - {zip_filename}\")\n",
        "    print(f\"   - {report_filename}\")\n",
        "\n",
        "print(f\"\\nüéâ TRAITEMENT TERMIN√â!\")\n",
        "print(f\"üìä Performance valid√©e: {len(documents)/total_time:.0f} docs/sec\")\n",
        "print(f\"üöÄ Pr√™t pour corpus PaniniFS complet avec vos 2To Google Drive!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
