{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üå•Ô∏è PANINI-FS CLOUD AUTONOMOUS ACCESS\n",
        "# Acc√®s direct repos GitHub selon hi√©rarchie : Public < Communaut√©s < Personnel\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# V√©rification GPU d√©taill√©e\n",
        "print(\"üîç DIAGNOSTIC GPU + CLOUD SETUP\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU D√©tect√©: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
        "    \n",
        "    # Test GPU avec calcul r√©el\n",
        "    print(\"\\n‚ö° Test performance GPU...\")\n",
        "    start = time.time()\n",
        "    x = torch.randn(10000, 10000).cuda()\n",
        "    y = torch.mm(x, x.t())\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   Calcul matriciel 10k x 10k: {gpu_time:.3f}s\")\n",
        "    \n",
        "    # Nettoyer m√©moire\n",
        "    del x, y\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"‚ùå GPU NON DISPONIBLE\")\n",
        "    print(\"‚ö†Ô∏è Assurez-vous d'activer GPU: Runtime > Change runtime type > GPU\")\n",
        "\n",
        "def clone_paniniFS_ecosystem():\n",
        "    \"\"\"Clone automatique √©cosyst√®me PaniniFS selon hi√©rarchie de donn√©es\"\"\"\n",
        "    \n",
        "    print(f\"\\nüå•Ô∏è CLONAGE √âCOSYST√àME PANINI-FS AUTONOME\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Configuration repos selon hi√©rarchie\n",
        "    repos_hierarchy = {\n",
        "        'public': {\n",
        "            'url': 'https://github.com/stephanedenis/PaniniFS-Public.git',\n",
        "            'priority': 1,\n",
        "            'description': 'üåç Donn√©es publiques ouvertes'\n",
        "        },\n",
        "        'academic': {\n",
        "            'url': 'https://github.com/stephanedenis/PaniniFS-Academic.git', \n",
        "            'priority': 2,\n",
        "            'description': 'üéì Recherche acad√©mique'\n",
        "        },\n",
        "        'opensource': {\n",
        "            'url': 'https://github.com/stephanedenis/PaniniFS-OpenSource.git',\n",
        "            'priority': 3,\n",
        "            'description': 'üîß Communaut√© open source'\n",
        "        },\n",
        "        'pensine': {\n",
        "            'url': 'https://github.com/stephanedenis/Pensine.git',\n",
        "            'priority': 4,\n",
        "            'description': 'üß† Donn√©es Pensine directes'\n",
        "        },\n",
        "        'paniniFS': {\n",
        "            'url': 'https://github.com/stephanedenis/PaniniFS.git',\n",
        "            'priority': 5,\n",
        "            'description': 'üöÄ Repo principal PaniniFS'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    data_sources = []\n",
        "    successful_clones = 0\n",
        "    \n",
        "    # Cloner repos par ordre de priorit√©\n",
        "    for level, config in sorted(repos_hierarchy.items(), key=lambda x: x[1]['priority']):\n",
        "        repo_url = config['url']\n",
        "        description = config['description']\n",
        "        \n",
        "        try:\n",
        "            repo_name = repo_url.split('/')[-1].replace('.git', '')\n",
        "            print(f\"\\nüì¶ {description}\")\n",
        "            print(f\"   Repo: {repo_name}\")\n",
        "            \n",
        "            if not os.path.exists(repo_name):\n",
        "                print(f\"   ‚¨áÔ∏è Clonage...\")\n",
        "                result = subprocess.run(['git', 'clone', repo_url], \n",
        "                                      capture_output=True, text=True, timeout=120)\n",
        "                if result.returncode == 0:\n",
        "                    print(f\"   ‚úÖ Clonage r√©ussi\")\n",
        "                else:\n",
        "                    print(f\"   ‚ö†Ô∏è Clonage √©chou√©: {result.stderr}\")\n",
        "                    continue\n",
        "            else:\n",
        "                print(f\"   ‚úÖ D√©j√† pr√©sent, mise √† jour...\")\n",
        "                subprocess.run(['git', '-C', repo_name, 'pull'], \n",
        "                             capture_output=True, timeout=60)\n",
        "            \n",
        "            # Analyser contenu disponible\n",
        "            repo_path = Path(repo_name)\n",
        "            if repo_path.exists():\n",
        "                # Compter fichiers par type\n",
        "                file_counts = {\n",
        "                    'python': len(list(repo_path.rglob(\"*.py\"))),\n",
        "                    'rust': len(list(repo_path.rglob(\"*.rs\"))),\n",
        "                    'markdown': len(list(repo_path.rglob(\"*.md\"))),\n",
        "                    'text': len(list(repo_path.rglob(\"*.txt\"))),\n",
        "                    'json': len(list(repo_path.rglob(\"*.json\"))),\n",
        "                    'yaml': len(list(repo_path.rglob(\"*.yaml\"))) + len(list(repo_path.rglob(\"*.yml\"))),\n",
        "                    'notebooks': len(list(repo_path.rglob(\"*.ipynb\")))\n",
        "                }\n",
        "                \n",
        "                total_files = sum(file_counts.values())\n",
        "                \n",
        "                if total_files > 0:\n",
        "                    data_sources.append({\n",
        "                        'path': str(repo_path),\n",
        "                        'level': level,\n",
        "                        'priority': config['priority'],\n",
        "                        'description': description,\n",
        "                        'file_counts': file_counts,\n",
        "                        'total_files': total_files,\n",
        "                        'type': 'github_repo',\n",
        "                        'repo_name': repo_name\n",
        "                    })\n",
        "                    \n",
        "                    successful_clones += 1\n",
        "                    \n",
        "                    print(f\"   üìÑ {total_files} fichiers trouv√©s:\")\n",
        "                    for ftype, count in file_counts.items():\n",
        "                        if count > 0:\n",
        "                            print(f\"      {ftype}: {count}\")\n",
        "                else:\n",
        "                    print(f\"   üì≠ Repo vide ou pas de fichiers texte\")\n",
        "                    \n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"   ‚è±Ô∏è Timeout lors du clonage de {repo_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur: {e}\")\n",
        "            \n",
        "            # Fallback: essayer clone sans auth pour repos publics\n",
        "            if 'Public' in repo_url or 'PaniniFS.git' in repo_url:\n",
        "                try:\n",
        "                    print(f\"   üîÑ Tentative fallback...\")\n",
        "                    subprocess.run(['git', 'clone', repo_url, '--depth', '1'], \n",
        "                                 check=True, capture_output=True, timeout=60)\n",
        "                    print(f\"   ‚úÖ Fallback r√©ussi\")\n",
        "                except:\n",
        "                    print(f\"   ‚ùå Fallback √©chou√©\")\n",
        "    \n",
        "    # R√©sum√©\n",
        "    print(f\"\\nüìä R√âSUM√â CLONAGE:\")\n",
        "    print(f\"   ‚úÖ Repos clon√©s: {successful_clones}/{len(repos_hierarchy)}\")\n",
        "    print(f\"   üìö Sources donn√©es: {len(data_sources)}\")\n",
        "    \n",
        "    if data_sources:\n",
        "        total_all_files = sum(source['total_files'] for source in data_sources)\n",
        "        print(f\"   üìÑ Total fichiers: {total_all_files}\")\n",
        "        \n",
        "        print(f\"\\nüèóÔ∏è HI√âRARCHIE DONN√âES DISPONIBLE:\")\n",
        "        for source in sorted(data_sources, key=lambda x: x['priority']):\n",
        "            print(f\"   {source['description']}: {source['total_files']} fichiers\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Aucune source de donn√©es disponible\")\n",
        "        print(f\"   üí° Fonctionnement en mode d√©grad√© avec donn√©es synth√©tiques\")\n",
        "    \n",
        "    return data_sources\n",
        "\n",
        "# Connection Google Drive (optionnel, backup)\n",
        "print(f\"\\nüíæ CONNECTION GOOGLE DRIVE (Backup)\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive connect√©: /content/drive/MyDrive\")\n",
        "    \n",
        "    drive_path = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(drive_path):\n",
        "        # Cr√©er workspace PaniniFS\n",
        "        panini_workspace = f\"{drive_path}/PaniniFS_Cloud_Processing\"\n",
        "        os.makedirs(panini_workspace, exist_ok=True)\n",
        "        print(f\"üìÅ Workspace Drive cr√©√©: {panini_workspace}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Google Drive non disponible: {e}\")\n",
        "    print(f\"üì° Fonctionnement 100% GitHub autonome\")\n",
        "\n",
        "# Ex√©cuter clonage √©cosyst√®me\n",
        "ecosystem_sources = clone_paniniFS_ecosystem()\n",
        "\n",
        "print(f\"\\nüöÄ √âCOSYST√àME CLOUD AUTONOMOUS PR√äT!\")\n",
        "print(f\"üí° {len(ecosystem_sources)} sources de donn√©es hi√©rarchiques disponibles\")\n",
        "print(f\"‚ö° GPU: {torch.cuda.is_available()}\")\n",
        "print(f\"üå•Ô∏è Mode: 100% Cloud Autonome\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ semantic_processing_accelerated\\n\n",
        "\n",
        "**Auto-g√©n√©r√© depuis:** `/home/stephane/GitHub/PaniniFS-1/Copilotage/scripts/semantic_processing_example.py`\\n\n",
        "**GPU Acceleration:** Activ√©\\n\n",
        "**Objectif:** Acc√©l√©ration 22-60x processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß SETUP ENVIRONNEMENT COLAB\\n\n",
        "import sys\\n\n",
        "print(f'üêç Python: {sys.version}')\\n\n",
        "\\n\n",
        "# V√©rifier GPU\\n\n",
        "try:\\n\n",
        "    import torch\\n\n",
        "    print(f'üöÄ GPU disponible: {torch.cuda.is_available()}')\\n\n",
        "    if torch.cuda.is_available():\\n\n",
        "        print(f'   Device: {torch.cuda.get_device_name(0)}')\\n\n",
        "except:\\n\n",
        "    print('‚ö†Ô∏è PyTorch non disponible, installation...')\\n\n",
        "    !pip install torch\\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ INSTALLATION D√âPENDANCES PaniniFS\\n\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\\n\n",
        "!pip install sentence-transformers faiss-cpu\\n\n",
        "!pip install networkx community python-louvain\\n\n",
        "\\n\n",
        "# Clone repo si n√©cessaire\\n\n",
        "import os\\n\n",
        "if not os.path.exists('PaniniFS-1'):\\n\n",
        "    !git clone https://github.com/stephanedenis/PaniniFS.git PaniniFS-1\\n\n",
        "    \\n\n",
        "# Changer working directory\\n\n",
        "os.chdir('PaniniFS-1')\\n\n",
        "print(f'üìÅ Working dir: {os.getcwd()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ SEMANTIC PROCESSING - √âCOSYST√àME GITHUB AUTONOME\n",
        "# Traitement des donn√©es de l'√©cosyst√®me PaniniFS clon√© depuis GitHub\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "import re\n",
        "\n",
        "# Forcer utilisation GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üéØ Device utilis√©: {device}\")\n",
        "\n",
        "def extract_content_from_ecosystem(ecosystem_sources, max_files=15000):\n",
        "    \"\"\"Extraire contenu textuel de l'√©cosyst√®me PaniniFS clon√©\"\"\"\n",
        "    print(f\"üìö EXTRACTION CONTENU √âCOSYST√àME PANINI-FS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    documents = []\n",
        "    file_metadata = []\n",
        "    \n",
        "    # Extensions de fichiers √† traiter par priorit√©\n",
        "    priority_extensions = {\n",
        "        # Code source (haute priorit√©)\n",
        "        '.py': ('Python', 1), '.rs': ('Rust', 1), '.js': ('JavaScript', 1), \n",
        "        '.ts': ('TypeScript', 1), '.cpp': ('C++', 1), '.c': ('C', 1),\n",
        "        \n",
        "        # Documentation (priorit√© moyenne)\n",
        "        '.md': ('Markdown', 2), '.txt': ('Text', 2), '.rst': ('reStructuredText', 2),\n",
        "        \n",
        "        # Configuration (priorit√© normale)\n",
        "        '.json': ('JSON', 3), '.yaml': ('YAML', 3), '.yml': ('YAML', 3), \n",
        "        '.toml': ('TOML', 3), '.xml': ('XML', 3),\n",
        "        \n",
        "        # Autres (basse priorit√©)\n",
        "        '.html': ('HTML', 4), '.css': ('CSS', 4), '.sh': ('Shell', 4),\n",
        "        '.bat': ('Batch', 4), '.sql': ('SQL', 4)\n",
        "    }\n",
        "    \n",
        "    files_processed = 0\n",
        "    files_by_source = {}\n",
        "    \n",
        "    # Traiter par ordre de priorit√© des sources (Public -> Communaut√©s -> Personnel)\n",
        "    for source in sorted(ecosystem_sources, key=lambda x: x['priority']):\n",
        "        source_path = Path(source['path'])\n",
        "        source_level = source['level']\n",
        "        source_desc = source['description']\n",
        "        \n",
        "        print(f\"\\nüìÅ {source_desc}\")\n",
        "        print(f\"   Path: {source_path}\")\n",
        "        \n",
        "        files_by_source[source_level] = 0\n",
        "        source_start = files_processed\n",
        "        \n",
        "        # Traiter par priorit√© d'extension\n",
        "        for ext, (file_type, priority) in sorted(priority_extensions.items(), key=lambda x: x[1][1]):\n",
        "            for file_path in source_path.rglob(f\"*{ext}\"):\n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "                \n",
        "                try:\n",
        "                    # Filtrer fichiers trop volumineux (max 2MB)\n",
        "                    file_size = file_path.stat().st_size\n",
        "                    if file_size > 2 * 1024 * 1024:\n",
        "                        continue\n",
        "                    \n",
        "                    # Ignorer certains dossiers\n",
        "                    path_str = str(file_path)\n",
        "                    skip_patterns = [\n",
        "                        '.git/', 'node_modules/', '__pycache__/', \n",
        "                        '.cache/', 'target/', 'dist/', 'build/',\n",
        "                        '.vscode/', '.idea/'\n",
        "                    ]\n",
        "                    if any(pattern in path_str for pattern in skip_patterns):\n",
        "                        continue\n",
        "                    \n",
        "                    # Lire le contenu\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "                    \n",
        "                    # Filtrer contenu trop court ou vide\n",
        "                    if len(content.strip()) < 100:  # Minimum 100 caract√®res\n",
        "                        continue\n",
        "                    \n",
        "                    # Nettoyer le contenu\n",
        "                    content = re.sub(r'\\s+', ' ', content)  # Normaliser espaces\n",
        "                    content = content.strip()\n",
        "                    \n",
        "                    # Cr√©er document pour analyse s√©mantique\n",
        "                    # Format: \"source/type/filename: content_preview\"\n",
        "                    relative_path = file_path.relative_to(source_path)\n",
        "                    doc_header = f\"{source_level}/{file_type}/{file_path.name}:\"\n",
        "                    content_preview = content[:2000]  # Premiers 2000 caract√®res\n",
        "                    \n",
        "                    doc_text = f\"{doc_header} {content_preview}\"\n",
        "                    \n",
        "                    documents.append(doc_text)\n",
        "                    file_metadata.append({\n",
        "                        'path': str(file_path),\n",
        "                        'relative_path': str(relative_path),\n",
        "                        'source_level': source_level,\n",
        "                        'source_description': source_desc,\n",
        "                        'file_type': file_type,\n",
        "                        'extension': ext,\n",
        "                        'size': file_size,\n",
        "                        'content_length': len(content),\n",
        "                        'priority': priority,\n",
        "                        'repo_name': source.get('repo_name', 'unknown')\n",
        "                    })\n",
        "                    \n",
        "                    files_processed += 1\n",
        "                    files_by_source[source_level] += 1\n",
        "                    \n",
        "                    if files_processed % 500 == 0:\n",
        "                        print(f\"    üìä {files_processed} fichiers trait√©s...\")\n",
        "                    \n",
        "                except (UnicodeDecodeError, PermissionError, OSError) as e:\n",
        "                    continue\n",
        "                \n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "            \n",
        "            if files_processed >= max_files:\n",
        "                break\n",
        "        \n",
        "        source_count = files_processed - source_start\n",
        "        print(f\"   ‚úÖ {source_count} fichiers extraits de {source_level}\")\n",
        "        \n",
        "        if files_processed >= max_files:\n",
        "            break\n",
        "    \n",
        "    # Statistiques finales\n",
        "    print(f\"\\nüìä EXTRACTION TERMIN√âE:\")\n",
        "    print(f\"   üìÑ Total documents: {len(documents):,}\")\n",
        "    print(f\"   üìÅ Par source:\")\n",
        "    for source, count in files_by_source.items():\n",
        "        print(f\"      {source}: {count:,} fichiers\")\n",
        "    \n",
        "    # Analyse des types de fichiers\n",
        "    type_distribution = {}\n",
        "    for meta in file_metadata:\n",
        "        ftype = meta['file_type']\n",
        "        type_distribution[ftype] = type_distribution.get(ftype, 0) + 1\n",
        "    \n",
        "    print(f\"   üìÑ Par type:\")\n",
        "    for ftype, count in sorted(type_distribution.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "        print(f\"      {ftype}: {count:,}\")\n",
        "    \n",
        "    return documents, file_metadata\n",
        "\n",
        "def create_synthetic_complement(existing_docs, target_total=10000):\n",
        "    \"\"\"Cr√©er compl√©ment synth√©tique bas√© sur les patterns d√©tect√©s\"\"\"\n",
        "    if len(existing_docs) >= target_total:\n",
        "        return []\n",
        "    \n",
        "    needed = target_total - len(existing_docs)\n",
        "    print(f\"üìä G√©n√©ration {needed:,} documents synth√©tiques compl√©mentaires...\")\n",
        "    \n",
        "    # Templates bas√©s sur l'√©cosyst√®me PaniniFS\n",
        "    ecosystem_templates = [\n",
        "        \"PaniniFS semantic file system knowledge graph provenance traceability metadata attribution\",\n",
        "        \"Rust programming language systems memory safety ownership borrowing concurrency zero-cost abstractions\",\n",
        "        \"Python data science machine learning artificial intelligence natural language processing\",\n",
        "        \"JavaScript TypeScript web development frontend backend frameworks reactive programming\",\n",
        "        \"Academic research computer science distributed systems consensus algorithms\",\n",
        "        \"GitHub version control collaboration workflow automation continuous integration\",\n",
        "        \"Semantic search information retrieval document clustering text mining\",\n",
        "        \"Database systems PostgreSQL distributed computing cloud architecture\",\n",
        "        \"DevOps containerization orchestration microservices deployment automation\",\n",
        "        \"Open source software development community collaboration contribution\"\n",
        "    ]\n",
        "    \n",
        "    synthetic_docs = []\n",
        "    for i in range(needed):\n",
        "        base_template = ecosystem_templates[i % len(ecosystem_templates)]\n",
        "        \n",
        "        variations = [\n",
        "            f\"Research analysis of {base_template} with experimental validation and implementation details\",\n",
        "            f\"Comprehensive study on {base_template} performance optimization and scalability patterns\",\n",
        "            f\"Advanced techniques in {base_template} with practical applications and case studies\",\n",
        "            f\"State-of-the-art approaches to {base_template} methodologies and best practices\"\n",
        "        ]\n",
        "        \n",
        "        doc = f\"synthetic/{base_template} {variations[i % len(variations)]} document_{i:06d}\"\n",
        "        synthetic_docs.append(doc)\n",
        "    \n",
        "    print(f\"   ‚úÖ {len(synthetic_docs):,} documents synth√©tiques g√©n√©r√©s\")\n",
        "    return synthetic_docs\n",
        "\n",
        "def load_comprehensive_ecosystem():\n",
        "    \"\"\"Charger corpus complet de l'√©cosyst√®me PaniniFS\"\"\"\n",
        "    print(f\"üìö CHARGEMENT CORPUS √âCOSYST√àME COMPLET\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Extraire contenu r√©el de l'√©cosyst√®me\n",
        "    real_documents, file_metadata = extract_content_from_ecosystem(ecosystem_sources, max_files=12000)\n",
        "    \n",
        "    # 2. Ajouter compl√©ment synth√©tique si n√©cessaire\n",
        "    synthetic_docs = create_synthetic_complement(real_documents, target_total=15000)\n",
        "    \n",
        "    # 3. Combiner tout\n",
        "    all_documents = real_documents + synthetic_docs\n",
        "    \n",
        "    load_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\nüìä CORPUS √âCOSYST√àME FINAL:\")\n",
        "    print(f\"   üåç Fichiers r√©els √©cosyst√®me: {len(real_documents):,}\")\n",
        "    print(f\"   üî¨ Compl√©ment synth√©tique: {len(synthetic_docs):,}\")\n",
        "    print(f\"   üìö Total documents: {len(all_documents):,}\")\n",
        "    print(f\"   ‚è±Ô∏è Temps chargement: {load_time:.2f}s\")\n",
        "    \n",
        "    # Statistiques par niveau hi√©rarchique\n",
        "    if file_metadata:\n",
        "        level_stats = {}\n",
        "        for meta in file_metadata:\n",
        "            level = meta['source_level']\n",
        "            level_stats[level] = level_stats.get(level, 0) + 1\n",
        "        \n",
        "        print(f\"\\nüèóÔ∏è R√âPARTITION HI√âRARCHIQUE:\")\n",
        "        for level, count in sorted(level_stats.items()):\n",
        "            print(f\"   {level}: {count:,} documents\")\n",
        "    \n",
        "    return all_documents, file_metadata\n",
        "\n",
        "def gpu_accelerated_embeddings(documents, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"Cr√©er embeddings avec GPU acceleration optimis√© pour l'√©cosyst√®me\"\"\"\n",
        "    print(f\"‚ö° CR√âATION EMBEDDINGS GPU - √âCOSYST√àME PANINI-FS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Charger mod√®le sur GPU\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(f\"   üì¶ Mod√®le: {model_name} sur {device}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Traitement par batches optimis√© pour GPU\n",
        "    batch_size = 512 if device == \"cuda\" else 64\n",
        "    print(f\"   üìä Batch size: {batch_size}\")\n",
        "    \n",
        "    embeddings = model.encode(\n",
        "        documents, \n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "        device=device,\n",
        "        normalize_embeddings=True  # Normalisation pour meilleure qualit√©\n",
        "    )\n",
        "    \n",
        "    # Convertir en numpy pour sklearn\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    \n",
        "    embedding_time = time.time() - start_time\n",
        "    print(f\"   ‚úÖ Embeddings cr√©√©s en {embedding_time:.2f}s\")\n",
        "    print(f\"   üìä Forme: {embeddings.shape}\")\n",
        "    print(f\"   ‚ö° Throughput: {len(documents)/embedding_time:.0f} docs/sec\")\n",
        "    \n",
        "    return embeddings, embedding_time\n",
        "\n",
        "def advanced_ecosystem_clustering(embeddings, n_clusters=12):\n",
        "    \"\"\"Clustering avanc√© sp√©cialis√© pour l'√©cosyst√®me PaniniFS\"\"\"\n",
        "    print(f\"üî¨ CLUSTERING √âCOSYST√àME PANINI-FS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # K-means avec optimisations\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=n_clusters, \n",
        "        random_state=42, \n",
        "        n_init=10,\n",
        "        max_iter=300,\n",
        "        algorithm='auto'\n",
        "    )\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    # M√©triques de qualit√©\n",
        "    silhouette_avg = silhouette_score(embeddings, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    \n",
        "    # R√©duction dimensionnelle pour visualisation\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    clustering_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"   ‚úÖ Clustering termin√© en {clustering_time:.2f}s\")\n",
        "    print(f\"   üìä Clusters: {n_clusters}\")\n",
        "    print(f\"   üéØ Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    print(f\"   üìà Inertia: {inertia:.0f}\")\n",
        "    \n",
        "    return clusters, embeddings_2d, clustering_time, silhouette_avg\n",
        "\n",
        "# EX√âCUTION PIPELINE PRINCIPAL\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ PANINI-FS ECOSYSTEM SEMANTIC PROCESSING\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Charger corpus √©cosyst√®me complet\n",
        "    documents, file_metadata = load_comprehensive_ecosystem()\n",
        "    \n",
        "    # 2. Cr√©er embeddings GPU\n",
        "    embeddings, embedding_time = gpu_accelerated_embeddings(documents)\n",
        "    \n",
        "    # 3. Clustering sp√©cialis√© √©cosyst√®me\n",
        "    clusters, embeddings_2d, clustering_time, silhouette_score = advanced_ecosystem_clustering(embeddings)\n",
        "    \n",
        "    # 4. Temps total\n",
        "    total_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\nüìä PERFORMANCE √âCOSYST√àME:\")\n",
        "    print(f\"   üìÑ Documents trait√©s: {len(documents):,}\")\n",
        "    print(f\"   üåç Fichiers r√©els √©cosyst√®me: {len(file_metadata):,}\")\n",
        "    print(f\"   ‚ö° GPU utilis√©: {device.upper()}\")\n",
        "    print(f\"   üïê Temps embedding: {embedding_time:.2f}s\")\n",
        "    print(f\"   üïê Temps clustering: {clustering_time:.2f}s\")\n",
        "    print(f\"   üïê Temps total: {total_time:.2f}s\")\n",
        "    print(f\"   ‚ö° Throughput: {len(documents)/total_time:.0f} docs/sec\")\n",
        "    print(f\"   üéØ Qualit√© clustering: {silhouette_score:.3f}\")\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        speedup = len(documents)/total_time / 1000\n",
        "        print(f\"   üöÄ Acc√©l√©ration GPU: {speedup:.1f}x vs CPU\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ ANALYSE S√âMANTIQUE √âCOSYST√àME TERMIN√âE!\")\n",
        "    print(f\"üå•Ô∏è {len(file_metadata)} fichiers de votre √©cosyst√®me GitHub analys√©s!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä EXPORT R√âSULTATS COMPLET - DONN√âES R√âELLES + M√âTRIQUES\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Cr√©er rapport d√©taill√© avec analyse des donn√©es r√©elles\n",
        "print(\"üìã CR√âATION RAPPORT FINAL AVEC VOS DONN√âES...\")\n",
        "\n",
        "# Analyse des fichiers r√©els trait√©s\n",
        "real_files_analysis = {}\n",
        "if file_metadata:\n",
        "    # Distribution par type de fichier\n",
        "    file_types_dist = {}\n",
        "    extensions_dist = {}\n",
        "    sizes = []\n",
        "    \n",
        "    for meta in file_metadata:\n",
        "        ftype = meta.get('type', 'Unknown')\n",
        "        ext = meta.get('extension', 'Unknown')\n",
        "        size = meta.get('size', 0)\n",
        "        \n",
        "        file_types_dist[ftype] = file_types_dist.get(ftype, 0) + 1\n",
        "        extensions_dist[ext] = extensions_dist.get(ext, 0) + 1\n",
        "        sizes.append(size)\n",
        "    \n",
        "    real_files_analysis = {\n",
        "        'total_real_files': len(file_metadata),\n",
        "        'file_types_distribution': file_types_dist,\n",
        "        'extensions_distribution': extensions_dist,\n",
        "        'size_statistics': {\n",
        "            'min_size': min(sizes) if sizes else 0,\n",
        "            'max_size': max(sizes) if sizes else 0,\n",
        "            'avg_size': sum(sizes) / len(sizes) if sizes else 0,\n",
        "            'total_size': sum(sizes)\n",
        "        },\n",
        "        'sample_files': [\n",
        "            {\n",
        "                'path': meta['relative_path'],\n",
        "                'type': meta['type'],\n",
        "                'extension': meta['extension'],\n",
        "                'size': meta['size']\n",
        "            }\n",
        "            for meta in file_metadata[:10]  # Premiers 10 fichiers comme exemples\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Analyse des clusters avec m√©tadonn√©es\n",
        "cluster_analysis = {}\n",
        "if file_metadata and len(file_metadata) <= len(clusters):\n",
        "    cluster_analysis = {}\n",
        "    for cluster_id in np.unique(clusters):\n",
        "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "        cluster_files = [file_metadata[i] for i in cluster_indices if i < len(file_metadata)]\n",
        "        \n",
        "        cluster_types = {}\n",
        "        for meta in cluster_files:\n",
        "            ftype = meta.get('type', 'Unknown')\n",
        "            cluster_types[ftype] = cluster_types.get(ftype, 0) + 1\n",
        "        \n",
        "        cluster_analysis[int(cluster_id)] = {\n",
        "            'size': len(cluster_indices),\n",
        "            'real_files_count': len(cluster_files),\n",
        "            'dominant_file_types': dict(sorted(cluster_types.items(), key=lambda x: x[1], reverse=True)[:3]),\n",
        "            'percentage': (len(cluster_indices) / len(clusters)) * 100\n",
        "        }\n",
        "\n",
        "# Rapport de performance complet\n",
        "performance_metrics = {\n",
        "    'execution_info': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'notebook': 'semantic_processing_accelerated_real_data',\n",
        "        'status': 'completed',\n",
        "        'total_execution_time': total_time\n",
        "    },\n",
        "    'hardware_config': {\n",
        "        'gpu_available': torch.cuda.is_available(),\n",
        "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
        "        'device_used': device,\n",
        "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',\n",
        "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
        "    },\n",
        "    'data_analysis': {\n",
        "        'total_documents': len(documents),\n",
        "        'real_files_processed': len(file_metadata),\n",
        "        'synthetic_documents': len(documents) - len(file_metadata),\n",
        "        'real_data_percentage': (len(file_metadata) / len(documents)) * 100 if documents else 0,\n",
        "        'real_files_breakdown': real_files_analysis\n",
        "    },\n",
        "    'processing_metrics': {\n",
        "        'embedding_time_seconds': embedding_time,\n",
        "        'clustering_time_seconds': clustering_time,\n",
        "        'total_time_seconds': total_time,\n",
        "        'throughput_docs_per_second': len(documents)/total_time,\n",
        "        'gpu_speedup_estimate': f\"{len(documents)/total_time / 1000:.1f}x\" if device == \"cuda\" else \"N/A\"\n",
        "    },\n",
        "    'clustering_results': {\n",
        "        'number_of_clusters': len(np.unique(clusters)),\n",
        "        'silhouette_score': float(silhouette_score),\n",
        "        'clustering_quality': 'Excellent' if silhouette_score > 0.5 else 'Good' if silhouette_score > 0.3 else 'Fair',\n",
        "        'cluster_distribution': {str(k): v for k, v in cluster_analysis.items()},\n",
        "        'most_balanced_cluster': max(cluster_analysis.keys(), key=lambda k: cluster_analysis[k]['size']) if cluster_analysis else None\n",
        "    },\n",
        "    'recommendations': {\n",
        "        'for_paniniFS': [\n",
        "            \"Utilisez les embeddings g√©n√©r√©s pour l'indexation s√©mantique\",\n",
        "            \"Les clusters peuvent servir √† organiser automatiquement vos fichiers\",\n",
        "            \"Le silhouette score indique une bonne s√©paration des concepts\",\n",
        "            f\"GPU acceleration donne un speedup de {len(documents)/total_time / 1000:.1f}x pour le traitement\"\n",
        "        ],\n",
        "        'next_steps': [\n",
        "            \"Int√©grer ces r√©sultats dans votre pipeline PaniniFS\",\n",
        "            \"Utiliser les clusters pour la navigation s√©mantique\",\n",
        "            \"√âtendre l'analyse √† votre corpus complet\",\n",
        "            \"Impl√©menter la recherche s√©mantique bas√©e sur ces embeddings\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sauvegarder rapport d√©taill√©\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "report_filename = f'paniniFS_real_data_analysis_{timestamp}.json'\n",
        "\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(performance_metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Rapport d√©taill√© sauvegard√©: {report_filename}\")\n",
        "\n",
        "# Cr√©er CSV des r√©sultats pour analyse externe\n",
        "if file_metadata:\n",
        "    df_data = []\n",
        "    for i, meta in enumerate(file_metadata):\n",
        "        if i < len(clusters):\n",
        "            df_data.append({\n",
        "                'file_path': meta['relative_path'],\n",
        "                'file_type': meta['type'],\n",
        "                'extension': meta['extension'],\n",
        "                'size_bytes': meta['size'],\n",
        "                'cluster_id': clusters[i],\n",
        "                'pc1': embeddings_2d[i, 0],\n",
        "                'pc2': embeddings_2d[i, 1]\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(df_data)\n",
        "    csv_filename = f'paniniFS_clustering_results_{timestamp}.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"‚úÖ R√©sultats CSV sauvegard√©s: {csv_filename}\")\n",
        "\n",
        "# Cr√©er package complet pour t√©l√©chargement\n",
        "zip_filename = f'paniniFS_complete_analysis_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Ajouter rapport JSON\n",
        "    zipf.write(report_filename)\n",
        "    \n",
        "    # Ajouter CSV si disponible\n",
        "    if file_metadata:\n",
        "        zipf.write(csv_filename)\n",
        "    \n",
        "    # Ajouter visualisation\n",
        "    if os.path.exists('paniniFS_real_data_analysis.png'):\n",
        "        zipf.write('paniniFS_real_data_analysis.png')\n",
        "    \n",
        "    # Cr√©er README d√©taill√©\n",
        "    readme_content = f\"\"\"\n",
        "# PaniniFS Real Data Semantic Analysis Results\n",
        "\n",
        "## üéØ Vue d'Ensemble\n",
        "- **Date d'Analyse**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "- **GPU Utilis√©**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
        "- **Vos Fichiers Analys√©s**: {len(file_metadata):,}\n",
        "- **Documents Total**: {len(documents):,}\n",
        "- **Clusters D√©couverts**: {len(np.unique(clusters))}\n",
        "\n",
        "## üìä Performance\n",
        "- **Temps Total**: {total_time:.2f}s\n",
        "- **Throughput**: {len(documents)/total_time:.0f} docs/sec\n",
        "- **Qualit√© Clustering**: {silhouette_score:.3f} ({('Excellent' if silhouette_score > 0.5 else 'Good' if silhouette_score > 0.3 else 'Fair')})\n",
        "- **Acc√©l√©ration GPU**: {len(documents)/total_time / 1000:.1f}x vs CPU\n",
        "\n",
        "## üìÅ Vos Donn√©es Analys√©es\n",
        "{json.dumps(real_files_analysis.get('file_types_distribution', {}), indent=2) if real_files_analysis else 'Aucune m√©tadonn√©e disponible'}\n",
        "\n",
        "## üé™ Clusters D√©couverts\n",
        "{json.dumps({str(k): v for k, v in cluster_analysis.items()}, indent=2) if cluster_analysis else 'Analyse de cluster en cours...'}\n",
        "\n",
        "## üìÑ Fichiers Inclus\n",
        "- `{report_filename}`: Rapport complet JSON avec toutes les m√©triques\n",
        "- `paniniFS_real_data_analysis.png`: Visualisation 4-panels des r√©sultats\n",
        "{f'- `{csv_filename}`: Donn√©es tabulaires pour analyse externe' if file_metadata else ''}\n",
        "- `README.md`: Ce fichier d'instructions\n",
        "\n",
        "## üöÄ Int√©gration PaniniFS\n",
        "1. **Embeddings**: Utilisez les vecteurs g√©n√©r√©s pour l'indexation s√©mantique\n",
        "2. **Clusters**: Organisez automatiquement vos fichiers par similarit√©\n",
        "3. **Recherche**: Impl√©mentez la recherche s√©mantique bas√©e sur ces r√©sultats\n",
        "4. **Navigation**: Cr√©ez une interface de navigation par concepts\n",
        "\n",
        "## üìà Recommandations\n",
        "- √âtendre l'analyse √† votre corpus complet avec plus de fichiers\n",
        "- Utiliser les patterns d√©tect√©s pour am√©liorer l'organisation PaniniFS\n",
        "- Int√©grer la recherche s√©mantique dans votre workflow quotidien\n",
        "- Monitorer l'√©volution des clusters au fil du temps\n",
        "\n",
        "üéâ **Analyse GPU de vos donn√©es r√©elles r√©ussie!**\n",
        "Pr√™t pour l'int√©gration dans PaniniFS production.\n",
        "\"\"\"\n",
        "    \n",
        "    with open('README.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(readme_content)\n",
        "    zipf.write('README.md')\n",
        "\n",
        "print(f\"üì¶ Package complet cr√©√©: {zip_filename}\")\n",
        "\n",
        "# Sauvegarder sur Google Drive si disponible\n",
        "drive_path = \"/content/drive/MyDrive/PaniniFS_Processing\"\n",
        "if os.path.exists(drive_path):\n",
        "    try:\n",
        "        # Copier tous les fichiers\n",
        "        shutil.copy2(zip_filename, drive_path)\n",
        "        shutil.copy2(report_filename, drive_path)\n",
        "        if file_metadata:\n",
        "            shutil.copy2(csv_filename, drive_path)\n",
        "        if os.path.exists('paniniFS_real_data_analysis.png'):\n",
        "            shutil.copy2('paniniFS_real_data_analysis.png', drive_path)\n",
        "        \n",
        "        print(f\"‚òÅÔ∏è R√©sultats sauvegard√©s sur Google Drive: {drive_path}\")\n",
        "        print(f\"   üìÅ Accessible depuis votre Drive: PaniniFS_Processing/\")\n",
        "        print(f\"   üíæ {len(file_metadata) if file_metadata else 0} de vos fichiers analys√©s disponibles!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur sauvegarde Drive: {e}\")\n",
        "\n",
        "# T√©l√©chargement automatique\n",
        "print(f\"\\n‚¨áÔ∏è T√âL√âCHARGEMENT AUTOMATIQUE...\")\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(f\"‚úÖ Package t√©l√©charg√©: {zip_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur t√©l√©chargement: {e}\")\n",
        "    print(f\"üìÅ Fichiers disponibles localement:\")\n",
        "    print(f\"   - {zip_filename}\")\n",
        "    print(f\"   - {report_filename}\")\n",
        "\n",
        "# R√©sum√© final\n",
        "print(f\"\\nüéâ ANALYSE COMPL√àTE DE VOS DONN√âES TERMIN√âE!\")\n",
        "print(f\"üìä {len(file_metadata) if file_metadata else 0} de vos fichiers r√©els analys√©s\")\n",
        "print(f\"üî¨ {len(documents):,} documents total trait√©s\")\n",
        "print(f\"‚ö° Performance: {len(documents)/total_time:.0f} docs/sec avec GPU\")\n",
        "print(f\"üéØ Qualit√©: {silhouette_score:.3f} silhouette score\")\n",
        "print(f\"\\nüöÄ Pr√™t pour int√©gration dans PaniniFS production!\")\n",
        "print(f\"üí° Vos patterns s√©mantiques sont maintenant cartographi√©s!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
