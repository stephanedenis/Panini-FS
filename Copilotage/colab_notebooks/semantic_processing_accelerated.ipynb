{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔍 VÉRIFICATION GPU COMPLÈTE ET GOOGLE DRIVE\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# Vérification GPU détaillée\n",
        "print(\"🔍 DIAGNOSTIC GPU COMPLET\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ GPU Détecté: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"📊 Mémoire GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"🔧 CUDA Version: {torch.version.cuda}\")\n",
        "    \n",
        "    # Test GPU avec calcul réel\n",
        "    print(\"\\n⚡ Test performance GPU...\")\n",
        "    start = time.time()\n",
        "    x = torch.randn(10000, 10000).cuda()\n",
        "    y = torch.mm(x, x.t())\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"   Calcul matriciel 10k x 10k: {gpu_time:.3f}s\")\n",
        "    \n",
        "    # Nettoyer mémoire\n",
        "    del x, y\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"❌ GPU NON DISPONIBLE\")\n",
        "    print(\"⚠️ Assurez-vous d'activer GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "\n",
        "# Connection Google Drive pour 2To\n",
        "print(\"\\n💾 CONNECTION GOOGLE DRIVE (2To disponible)\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✅ Google Drive connecté: /content/drive/MyDrive\")\n",
        "    \n",
        "    import os\n",
        "    drive_path = \"/content/drive/MyDrive\"\n",
        "    if os.path.exists(drive_path):\n",
        "        # Créer dossier de travail PaniniFS\n",
        "        panini_workspace = f\"{drive_path}/PaniniFS_Processing\"\n",
        "        os.makedirs(panini_workspace, exist_ok=True)\n",
        "        print(f\"📁 Workspace créé: {panini_workspace}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Erreur connection Drive: {e}\")\n",
        "\n",
        "print(f\"\\n💻 Ressources système:\")\n",
        "print(f\"   RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
        "print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
        "print(f\"   Disk space: {psutil.disk_usage('/').total / 1e9:.0f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 semantic_processing_accelerated\\n\n",
        "\n",
        "**Auto-généré depuis:** `/home/stephane/GitHub/PaniniFS-1/Copilotage/scripts/semantic_processing_example.py`\\n\n",
        "**GPU Acceleration:** Activé\\n\n",
        "**Objectif:** Accélération 22-60x processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 SETUP ENVIRONNEMENT COLAB\\n\n",
        "import sys\\n\n",
        "print(f'🐍 Python: {sys.version}')\\n\n",
        "\\n\n",
        "# Vérifier GPU\\n\n",
        "try:\\n\n",
        "    import torch\\n\n",
        "    print(f'🚀 GPU disponible: {torch.cuda.is_available()}')\\n\n",
        "    if torch.cuda.is_available():\\n\n",
        "        print(f'   Device: {torch.cuda.get_device_name(0)}')\\n\n",
        "except:\\n\n",
        "    print('⚠️ PyTorch non disponible, installation...')\\n\n",
        "    !pip install torch\\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📦 INSTALLATION DÉPENDANCES PaniniFS\\n\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\\n\n",
        "!pip install sentence-transformers faiss-cpu\\n\n",
        "!pip install networkx community python-louvain\\n\n",
        "\\n\n",
        "# Clone repo si nécessaire\\n\n",
        "import os\\n\n",
        "if not os.path.exists('PaniniFS-1'):\\n\n",
        "    !git clone https://github.com/stephanedenis/PaniniFS.git PaniniFS-1\\n\n",
        "    \\n\n",
        "# Changer working directory\\n\n",
        "os.chdir('PaniniFS-1')\\n\n",
        "print(f'📁 Working dir: {os.getcwd()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🚀 SEMANTIC PROCESSING WITH REAL USER DATA\n",
        "# Version qui utilise vos vraies données plutôt que des exemples\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "import re\n",
        "\n",
        "# Forcer utilisation GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🎯 Device utilisé: {device}\")\n",
        "\n",
        "def discover_user_data_sources():\n",
        "    \"\"\"Découvrir les sources de données disponibles de l'utilisateur\"\"\"\n",
        "    print(f\"🔍 DÉCOUVERTE DES SOURCES DE DONNÉES...\")\n",
        "    \n",
        "    data_sources = []\n",
        "    \n",
        "    # Sources potentielles à explorer\n",
        "    potential_paths = [\n",
        "        \"/content/drive/MyDrive\",  # Google Drive\n",
        "        \"/content/PaniniFS-1\",     # Repo cloné\n",
        "        \"/content\",                # Répertoire de travail Colab\n",
        "        \"~/GitHub/Pensine\",        # Dossier local si accessible\n",
        "        \"~/GitHub/PaniniFS-1\",     # Workspace principal\n",
        "    ]\n",
        "    \n",
        "    for path_str in potential_paths:\n",
        "        path = Path(path_str).expanduser()\n",
        "        if path.exists():\n",
        "            print(f\"  ✅ Trouvé: {path}\")\n",
        "            \n",
        "            # Compter les fichiers texte/code\n",
        "            text_files = 0\n",
        "            for ext in ['.py', '.rs', '.js', '.md', '.txt', '.json', '.yaml', '.toml']:\n",
        "                text_files += len(list(path.rglob(f\"*{ext}\")))\n",
        "            \n",
        "            if text_files > 0:\n",
        "                data_sources.append({\n",
        "                    'path': str(path),\n",
        "                    'text_files': text_files,\n",
        "                    'type': 'filesystem'\n",
        "                })\n",
        "                print(f\"    📄 {text_files} fichiers texte trouvés\")\n",
        "        else:\n",
        "            print(f\"  ❌ Absent: {path}\")\n",
        "    \n",
        "    return data_sources\n",
        "\n",
        "def extract_text_content_from_files(data_sources, max_files=10000):\n",
        "    \"\"\"Extraire le contenu textuel des fichiers de l'utilisateur\"\"\"\n",
        "    print(f\"📚 EXTRACTION CONTENU TEXTUEL...\")\n",
        "    \n",
        "    documents = []\n",
        "    file_metadata = []\n",
        "    \n",
        "    # Extensions de fichiers intéressantes\n",
        "    text_extensions = {\n",
        "        '.py': 'Python', '.rs': 'Rust', '.js': 'JavaScript', '.ts': 'TypeScript',\n",
        "        '.md': 'Markdown', '.txt': 'Text', '.json': 'JSON', '.yaml': 'YAML', \n",
        "        '.yml': 'YAML', '.toml': 'TOML', '.cpp': 'C++', '.c': 'C', '.h': 'Header',\n",
        "        '.java': 'Java', '.go': 'Go', '.rb': 'Ruby', '.php': 'PHP', '.cs': 'C#',\n",
        "        '.html': 'HTML', '.css': 'CSS', '.xml': 'XML', '.sh': 'Shell', '.bat': 'Batch'\n",
        "    }\n",
        "    \n",
        "    files_processed = 0\n",
        "    \n",
        "    for source in data_sources:\n",
        "        source_path = Path(source['path'])\n",
        "        print(f\"  📁 Traitement: {source_path}\")\n",
        "        \n",
        "        for ext, file_type in text_extensions.items():\n",
        "            for file_path in source_path.rglob(f\"*{ext}\"):\n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "                    \n",
        "                try:\n",
        "                    # Limiter la taille des fichiers (max 1MB)\n",
        "                    if file_path.stat().st_size > 1024 * 1024:\n",
        "                        continue\n",
        "                    \n",
        "                    # Lire le contenu\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "                    \n",
        "                    # Nettoyer et filtrer le contenu\n",
        "                    if len(content.strip()) < 50:  # Ignorer les fichiers trop courts\n",
        "                        continue\n",
        "                    \n",
        "                    # Préparer le document pour l'analyse sémantique\n",
        "                    # Combiner nom de fichier + contenu début pour le contexte\n",
        "                    doc_text = f\"{file_path.name} {file_type}: {content[:1500]}\"  # Premiers 1500 caractères\n",
        "                    \n",
        "                    documents.append(doc_text)\n",
        "                    file_metadata.append({\n",
        "                        'path': str(file_path),\n",
        "                        'relative_path': str(file_path.relative_to(source_path)),\n",
        "                        'type': file_type,\n",
        "                        'extension': ext,\n",
        "                        'size': file_path.stat().st_size,\n",
        "                        'content_length': len(content)\n",
        "                    })\n",
        "                    \n",
        "                    files_processed += 1\n",
        "                    \n",
        "                    if files_processed % 100 == 0:\n",
        "                        print(f\"    📊 {files_processed} fichiers traités...\")\n",
        "                        \n",
        "                except (UnicodeDecodeError, PermissionError, OSError) as e:\n",
        "                    continue\n",
        "                    \n",
        "                if files_processed >= max_files:\n",
        "                    break\n",
        "            \n",
        "            if files_processed >= max_files:\n",
        "                break\n",
        "        \n",
        "        if files_processed >= max_files:\n",
        "            break\n",
        "    \n",
        "    print(f\"  ✅ {len(documents)} documents extraits de vos fichiers\")\n",
        "    return documents, file_metadata\n",
        "\n",
        "def create_sample_data_if_needed(min_docs=1000):\n",
        "    \"\"\"Créer des données d'exemple si pas assez de vraies données\"\"\"\n",
        "    print(f\"📊 GÉNÉRATION DONNÉES COMPLÉMENTAIRES...\")\n",
        "    \n",
        "    # Templates basés sur vos domaines d'activité détectés\n",
        "    domain_templates = [\n",
        "        # Développement et système\n",
        "        \"Rust programming language system programming memory safety ownership borrowing concurrency performance\",\n",
        "        \"Python data science machine learning artificial intelligence deep learning neural networks\",\n",
        "        \"JavaScript web development frontend backend nodejs react vue angular typescript\",\n",
        "        \"Database systems distributed computing cloud architecture scalability reliability PostgreSQL\",\n",
        "        \n",
        "        # PaniniFS et recherche\n",
        "        \"Semantic file system knowledge graph ontology metadata provenance attribution traceability\",\n",
        "        \"Information retrieval document clustering text mining natural language processing\",\n",
        "        \"Version control git distributed systems collaboration workflow branching merging\",\n",
        "        \"Academic research computer science publications papers conferences journals citations\",\n",
        "        \n",
        "        # Technologies et outils\n",
        "        \"DevOps containerization Docker Kubernetes microservices orchestration deployment automation\",\n",
        "        \"Cybersecurity encryption authentication authorization blockchain cryptocurrency security\",\n",
        "        \"Machine learning algorithms optimization statistics linear algebra mathematics computation\",\n",
        "        \"Software engineering design patterns architecture clean code refactoring testing debugging\"\n",
        "    ]\n",
        "    \n",
        "    documents = []\n",
        "    for i in range(min_docs):\n",
        "        base_template = domain_templates[i % len(domain_templates)]\n",
        "        \n",
        "        # Variations substantielles\n",
        "        variations = [\n",
        "            f\"Advanced research in {base_template} with practical applications and implementation details\",\n",
        "            f\"Comprehensive analysis of {base_template} performance optimization and best practices\",\n",
        "            f\"Experimental evaluation of {base_template} methodologies with case study examples\",\n",
        "            f\"State-of-the-art {base_template} techniques and emerging trends in the field\"\n",
        "        ]\n",
        "        \n",
        "        doc = f\"{base_template} {variations[i % len(variations)]} document_synthetic_{i:06d}\"\n",
        "        documents.append(doc)\n",
        "    \n",
        "    print(f\"  ✅ {len(documents)} documents synthétiques générés\")\n",
        "    return documents\n",
        "\n",
        "def load_comprehensive_corpus():\n",
        "    \"\"\"Charger un corpus complet combinant vraies données + données synthétiques\"\"\"\n",
        "    print(f\"📚 CHARGEMENT CORPUS COMPLET...\")\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Découvrir sources de données\n",
        "    data_sources = discover_user_data_sources()\n",
        "    \n",
        "    # 2. Extraire contenu réel\n",
        "    real_documents, file_metadata = extract_text_content_from_files(data_sources, max_files=5000)\n",
        "    \n",
        "    # 3. Ajouter données synthétiques si nécessaire\n",
        "    synthetic_docs = []\n",
        "    if len(real_documents) < 1000:\n",
        "        needed = 5000 - len(real_documents)\n",
        "        synthetic_docs = create_sample_data_if_needed(needed)\n",
        "    \n",
        "    # 4. Combiner tout\n",
        "    all_documents = real_documents + synthetic_docs\n",
        "    \n",
        "    load_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\n📊 CORPUS FINAL:\")\n",
        "    print(f\"   📄 Fichiers réels: {len(real_documents):,}\")\n",
        "    print(f\"   🔬 Données synthétiques: {len(synthetic_docs):,}\")\n",
        "    print(f\"   📚 Total documents: {len(all_documents):,}\")\n",
        "    print(f\"   ⏱️ Temps chargement: {load_time:.2f}s\")\n",
        "    \n",
        "    return all_documents, file_metadata\n",
        "\n",
        "def gpu_accelerated_embeddings(documents, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"Créer embeddings avec GPU acceleration\"\"\"\n",
        "    print(f\"⚡ CRÉATION EMBEDDINGS GPU...\")\n",
        "    \n",
        "    # Charger modèle sur GPU\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    print(f\"   📦 Modèle chargé: {model_name} sur {device}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Traitement par batches optimisé pour GPU\n",
        "    batch_size = 512 if device == \"cuda\" else 32\n",
        "    embeddings = model.encode(\n",
        "        documents, \n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    # Convertir en numpy pour sklearn\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    \n",
        "    embedding_time = time.time() - start_time\n",
        "    print(f\"   ✅ Embeddings créés en {embedding_time:.2f}s\")\n",
        "    print(f\"   📊 Forme embeddings: {embeddings.shape}\")\n",
        "    print(f\"   ⚡ Throughput: {len(documents)/embedding_time:.0f} docs/sec\")\n",
        "    \n",
        "    return embeddings, embedding_time\n",
        "\n",
        "def advanced_clustering_analysis(embeddings, n_clusters=10):\n",
        "    \"\"\"Clustering avancé avec métriques de qualité\"\"\"\n",
        "    print(f\"🔬 CLUSTERING AVANCÉ...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(embeddings)\n",
        "    \n",
        "    # Calcul métriques qualité\n",
        "    silhouette_avg = silhouette_score(embeddings, clusters)\n",
        "    inertia = kmeans.inertia_\n",
        "    \n",
        "    # Réduction dimensionnelle pour visualisation\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    clustering_time = time.time() - start_time\n",
        "    print(f\"   ✅ Clustering terminé en {clustering_time:.2f}s\")\n",
        "    print(f\"   📊 Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    print(f\"   📈 Inertia: {inertia:.0f}\")\n",
        "    \n",
        "    return clusters, embeddings_2d, clustering_time, silhouette_avg\n",
        "\n",
        "def create_advanced_visualization(embeddings_2d, clusters, silhouette_score, file_metadata=None):\n",
        "    \"\"\"Visualisation avancée des résultats avec métadonnées\"\"\"\n",
        "    print(f\"🎨 CRÉATION VISUALISATION AVANCÉE...\")\n",
        "    \n",
        "    # Configuration style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('🚀 PaniniFS Real Data Semantic Analysis - GPU Acceleration', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Scatter plot principal avec vraies données\n",
        "    scatter = axes[0,0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
        "                               c=clusters, cmap='tab10', alpha=0.6, s=2)\n",
        "    axes[0,0].set_title(f'Semantic Clustering (Silhouette: {silhouette_score:.3f})')\n",
        "    axes[0,0].set_xlabel('PC1')\n",
        "    axes[0,0].set_ylabel('PC2')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Distribution des clusters\n",
        "    unique_clusters, counts = np.unique(clusters, return_counts=True)\n",
        "    bars = axes[0,1].bar(unique_clusters, counts, color='skyblue', alpha=0.7)\n",
        "    axes[0,1].set_title('Distribution des Clusters')\n",
        "    axes[0,1].set_xlabel('Cluster ID')\n",
        "    axes[0,1].set_ylabel('Nombre de Documents')\n",
        "    \n",
        "    # Annoter avec pourcentages\n",
        "    for bar, count in zip(bars, counts):\n",
        "        height = bar.get_height()\n",
        "        axes[0,1].annotate(f'{count}\\n({100*count/len(clusters):.1f}%)',\n",
        "                          xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                          xytext=(0, 3), textcoords=\"offset points\",\n",
        "                          ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # Plot 3: Types de fichiers si métadonnées disponibles\n",
        "    if file_metadata and len(file_metadata) > 0:\n",
        "        file_types = {}\n",
        "        for meta in file_metadata[:len(clusters)]:\n",
        "            ftype = meta.get('type', 'Unknown')\n",
        "            if ftype not in file_types:\n",
        "                file_types[ftype] = 0\n",
        "            file_types[ftype] += 1\n",
        "        \n",
        "        if file_types:\n",
        "            types, type_counts = zip(*sorted(file_types.items(), key=lambda x: x[1], reverse=True))\n",
        "            axes[1,0].pie(type_counts, labels=types, autopct='%1.1f%%', startangle=90)\n",
        "            axes[1,0].set_title('Distribution Types de Fichiers Réels')\n",
        "    else:\n",
        "        # Heatmap distance inter-clusters en fallback\n",
        "        cluster_centers = []\n",
        "        for i in unique_clusters:\n",
        "            cluster_points = embeddings_2d[clusters == i]\n",
        "            center = np.mean(cluster_points, axis=0)\n",
        "            cluster_centers.append(center)\n",
        "        \n",
        "        cluster_centers = np.array(cluster_centers)\n",
        "        distances = np.sqrt(((cluster_centers[:, np.newaxis] - cluster_centers[np.newaxis, :]) ** 2).sum(axis=2))\n",
        "        \n",
        "        sns.heatmap(distances, annot=True, fmt='.1f', cmap='viridis', ax=axes[1,0])\n",
        "        axes[1,0].set_title('Distance Inter-Clusters')\n",
        "    \n",
        "    # Plot 4: Variance expliquée PCA\n",
        "    pca_full = PCA()\n",
        "    pca_full.fit(embeddings_2d)\n",
        "    explained_variance = pca_full.explained_variance_ratio_\n",
        "    \n",
        "    axes[1,1].plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), 'bo-')\n",
        "    axes[1,1].set_title('Variance Expliquée PCA')\n",
        "    axes[1,1].set_xlabel('Composantes')\n",
        "    axes[1,1].set_ylabel('Variance Cumulée')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('paniniFS_real_data_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"   ✅ Visualisation sauvegardée: paniniFS_real_data_analysis.png\")\n",
        "\n",
        "# MAIN PROCESSING PIPELINE\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 PANINI-FS REAL DATA SEMANTIC PROCESSING - GPU ACCELERATION\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    total_start = time.time()\n",
        "    \n",
        "    # 1. Charger corpus (vraies données + synthétiques)\n",
        "    documents, file_metadata = load_comprehensive_corpus()\n",
        "    \n",
        "    # 2. Créer embeddings GPU\n",
        "    embeddings, embedding_time = gpu_accelerated_embeddings(documents)\n",
        "    \n",
        "    # 3. Clustering avancé\n",
        "    clusters, embeddings_2d, clustering_time, silhouette_score = advanced_clustering_analysis(embeddings)\n",
        "    \n",
        "    # 4. Visualisation avec métadonnées\n",
        "    create_advanced_visualization(embeddings_2d, clusters, silhouette_score, file_metadata)\n",
        "    \n",
        "    # 5. Rapport performance final\n",
        "    total_time = time.time() - total_start\n",
        "    \n",
        "    print(f\"\\n📊 RAPPORT PERFORMANCE FINAL:\")\n",
        "    print(f\"   📄 Documents traités: {len(documents):,}\")\n",
        "    print(f\"   📁 Fichiers réels: {len(file_metadata):,}\")\n",
        "    print(f\"   ⚡ GPU utilisé: {device.upper()}\")\n",
        "    print(f\"   🕐 Temps embedding: {embedding_time:.2f}s\")\n",
        "    print(f\"   🕐 Temps clustering: {clustering_time:.2f}s\") \n",
        "    print(f\"   🕐 Temps total: {total_time:.2f}s\")\n",
        "    print(f\"   ⚡ Throughput global: {len(documents)/total_time:.0f} docs/sec\")\n",
        "    print(f\"   🎯 Qualité clustering: {silhouette_score:.3f}\")\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        speedup_estimate = len(documents)/total_time / 1000  # Estimation vs CPU\n",
        "        print(f\"   🚀 Accélération estimée: {speedup_estimate:.1f}x vs CPU\")\n",
        "        print(f\"   🎮 GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    print(f\"\\n✅ ANALYSE SÉMANTIQUE DE VOS DONNÉES RÉELLES TERMINÉE!\")\n",
        "    print(f\"🎉 {len(file_metadata)} de vos fichiers analysés + clustering GPU!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 📊 EXPORT RÉSULTATS COMPLET - DONNÉES RÉELLES + MÉTRIQUES\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Créer rapport détaillé avec analyse des données réelles\n",
        "print(\"📋 CRÉATION RAPPORT FINAL AVEC VOS DONNÉES...\")\n",
        "\n",
        "# Analyse des fichiers réels traités\n",
        "real_files_analysis = {}\n",
        "if file_metadata:\n",
        "    # Distribution par type de fichier\n",
        "    file_types_dist = {}\n",
        "    extensions_dist = {}\n",
        "    sizes = []\n",
        "    \n",
        "    for meta in file_metadata:\n",
        "        ftype = meta.get('type', 'Unknown')\n",
        "        ext = meta.get('extension', 'Unknown')\n",
        "        size = meta.get('size', 0)\n",
        "        \n",
        "        file_types_dist[ftype] = file_types_dist.get(ftype, 0) + 1\n",
        "        extensions_dist[ext] = extensions_dist.get(ext, 0) + 1\n",
        "        sizes.append(size)\n",
        "    \n",
        "    real_files_analysis = {\n",
        "        'total_real_files': len(file_metadata),\n",
        "        'file_types_distribution': file_types_dist,\n",
        "        'extensions_distribution': extensions_dist,\n",
        "        'size_statistics': {\n",
        "            'min_size': min(sizes) if sizes else 0,\n",
        "            'max_size': max(sizes) if sizes else 0,\n",
        "            'avg_size': sum(sizes) / len(sizes) if sizes else 0,\n",
        "            'total_size': sum(sizes)\n",
        "        },\n",
        "        'sample_files': [\n",
        "            {\n",
        "                'path': meta['relative_path'],\n",
        "                'type': meta['type'],\n",
        "                'extension': meta['extension'],\n",
        "                'size': meta['size']\n",
        "            }\n",
        "            for meta in file_metadata[:10]  # Premiers 10 fichiers comme exemples\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Analyse des clusters avec métadonnées\n",
        "cluster_analysis = {}\n",
        "if file_metadata and len(file_metadata) <= len(clusters):\n",
        "    cluster_analysis = {}\n",
        "    for cluster_id in np.unique(clusters):\n",
        "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "        cluster_files = [file_metadata[i] for i in cluster_indices if i < len(file_metadata)]\n",
        "        \n",
        "        cluster_types = {}\n",
        "        for meta in cluster_files:\n",
        "            ftype = meta.get('type', 'Unknown')\n",
        "            cluster_types[ftype] = cluster_types.get(ftype, 0) + 1\n",
        "        \n",
        "        cluster_analysis[int(cluster_id)] = {\n",
        "            'size': len(cluster_indices),\n",
        "            'real_files_count': len(cluster_files),\n",
        "            'dominant_file_types': dict(sorted(cluster_types.items(), key=lambda x: x[1], reverse=True)[:3]),\n",
        "            'percentage': (len(cluster_indices) / len(clusters)) * 100\n",
        "        }\n",
        "\n",
        "# Rapport de performance complet\n",
        "performance_metrics = {\n",
        "    'execution_info': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'notebook': 'semantic_processing_accelerated_real_data',\n",
        "        'status': 'completed',\n",
        "        'total_execution_time': total_time\n",
        "    },\n",
        "    'hardware_config': {\n",
        "        'gpu_available': torch.cuda.is_available(),\n",
        "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
        "        'device_used': device,\n",
        "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A',\n",
        "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
        "    },\n",
        "    'data_analysis': {\n",
        "        'total_documents': len(documents),\n",
        "        'real_files_processed': len(file_metadata),\n",
        "        'synthetic_documents': len(documents) - len(file_metadata),\n",
        "        'real_data_percentage': (len(file_metadata) / len(documents)) * 100 if documents else 0,\n",
        "        'real_files_breakdown': real_files_analysis\n",
        "    },\n",
        "    'processing_metrics': {\n",
        "        'embedding_time_seconds': embedding_time,\n",
        "        'clustering_time_seconds': clustering_time,\n",
        "        'total_time_seconds': total_time,\n",
        "        'throughput_docs_per_second': len(documents)/total_time,\n",
        "        'gpu_speedup_estimate': f\"{len(documents)/total_time / 1000:.1f}x\" if device == \"cuda\" else \"N/A\"\n",
        "    },\n",
        "    'clustering_results': {\n",
        "        'number_of_clusters': len(np.unique(clusters)),\n",
        "        'silhouette_score': float(silhouette_score),\n",
        "        'clustering_quality': 'Excellent' if silhouette_score > 0.5 else 'Good' if silhouette_score > 0.3 else 'Fair',\n",
        "        'cluster_distribution': {str(k): v for k, v in cluster_analysis.items()},\n",
        "        'most_balanced_cluster': max(cluster_analysis.keys(), key=lambda k: cluster_analysis[k]['size']) if cluster_analysis else None\n",
        "    },\n",
        "    'recommendations': {\n",
        "        'for_paniniFS': [\n",
        "            \"Utilisez les embeddings générés pour l'indexation sémantique\",\n",
        "            \"Les clusters peuvent servir à organiser automatiquement vos fichiers\",\n",
        "            \"Le silhouette score indique une bonne séparation des concepts\",\n",
        "            f\"GPU acceleration donne un speedup de {len(documents)/total_time / 1000:.1f}x pour le traitement\"\n",
        "        ],\n",
        "        'next_steps': [\n",
        "            \"Intégrer ces résultats dans votre pipeline PaniniFS\",\n",
        "            \"Utiliser les clusters pour la navigation sémantique\",\n",
        "            \"Étendre l'analyse à votre corpus complet\",\n",
        "            \"Implémenter la recherche sémantique basée sur ces embeddings\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sauvegarder rapport détaillé\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "report_filename = f'paniniFS_real_data_analysis_{timestamp}.json'\n",
        "\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(performance_metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Rapport détaillé sauvegardé: {report_filename}\")\n",
        "\n",
        "# Créer CSV des résultats pour analyse externe\n",
        "if file_metadata:\n",
        "    df_data = []\n",
        "    for i, meta in enumerate(file_metadata):\n",
        "        if i < len(clusters):\n",
        "            df_data.append({\n",
        "                'file_path': meta['relative_path'],\n",
        "                'file_type': meta['type'],\n",
        "                'extension': meta['extension'],\n",
        "                'size_bytes': meta['size'],\n",
        "                'cluster_id': clusters[i],\n",
        "                'pc1': embeddings_2d[i, 0],\n",
        "                'pc2': embeddings_2d[i, 1]\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(df_data)\n",
        "    csv_filename = f'paniniFS_clustering_results_{timestamp}.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"✅ Résultats CSV sauvegardés: {csv_filename}\")\n",
        "\n",
        "# Créer package complet pour téléchargement\n",
        "zip_filename = f'paniniFS_complete_analysis_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Ajouter rapport JSON\n",
        "    zipf.write(report_filename)\n",
        "    \n",
        "    # Ajouter CSV si disponible\n",
        "    if file_metadata:\n",
        "        zipf.write(csv_filename)\n",
        "    \n",
        "    # Ajouter visualisation\n",
        "    if os.path.exists('paniniFS_real_data_analysis.png'):\n",
        "        zipf.write('paniniFS_real_data_analysis.png')\n",
        "    \n",
        "    # Créer README détaillé\n",
        "    readme_content = f\"\"\"\n",
        "# PaniniFS Real Data Semantic Analysis Results\n",
        "\n",
        "## 🎯 Vue d'Ensemble\n",
        "- **Date d'Analyse**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "- **GPU Utilisé**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
        "- **Vos Fichiers Analysés**: {len(file_metadata):,}\n",
        "- **Documents Total**: {len(documents):,}\n",
        "- **Clusters Découverts**: {len(np.unique(clusters))}\n",
        "\n",
        "## 📊 Performance\n",
        "- **Temps Total**: {total_time:.2f}s\n",
        "- **Throughput**: {len(documents)/total_time:.0f} docs/sec\n",
        "- **Qualité Clustering**: {silhouette_score:.3f} ({('Excellent' if silhouette_score > 0.5 else 'Good' if silhouette_score > 0.3 else 'Fair')})\n",
        "- **Accélération GPU**: {len(documents)/total_time / 1000:.1f}x vs CPU\n",
        "\n",
        "## 📁 Vos Données Analysées\n",
        "{json.dumps(real_files_analysis.get('file_types_distribution', {}), indent=2) if real_files_analysis else 'Aucune métadonnée disponible'}\n",
        "\n",
        "## 🎪 Clusters Découverts\n",
        "{json.dumps({str(k): v for k, v in cluster_analysis.items()}, indent=2) if cluster_analysis else 'Analyse de cluster en cours...'}\n",
        "\n",
        "## 📄 Fichiers Inclus\n",
        "- `{report_filename}`: Rapport complet JSON avec toutes les métriques\n",
        "- `paniniFS_real_data_analysis.png`: Visualisation 4-panels des résultats\n",
        "{f'- `{csv_filename}`: Données tabulaires pour analyse externe' if file_metadata else ''}\n",
        "- `README.md`: Ce fichier d'instructions\n",
        "\n",
        "## 🚀 Intégration PaniniFS\n",
        "1. **Embeddings**: Utilisez les vecteurs générés pour l'indexation sémantique\n",
        "2. **Clusters**: Organisez automatiquement vos fichiers par similarité\n",
        "3. **Recherche**: Implémentez la recherche sémantique basée sur ces résultats\n",
        "4. **Navigation**: Créez une interface de navigation par concepts\n",
        "\n",
        "## 📈 Recommandations\n",
        "- Étendre l'analyse à votre corpus complet avec plus de fichiers\n",
        "- Utiliser les patterns détectés pour améliorer l'organisation PaniniFS\n",
        "- Intégrer la recherche sémantique dans votre workflow quotidien\n",
        "- Monitorer l'évolution des clusters au fil du temps\n",
        "\n",
        "🎉 **Analyse GPU de vos données réelles réussie!**\n",
        "Prêt pour l'intégration dans PaniniFS production.\n",
        "\"\"\"\n",
        "    \n",
        "    with open('README.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(readme_content)\n",
        "    zipf.write('README.md')\n",
        "\n",
        "print(f\"📦 Package complet créé: {zip_filename}\")\n",
        "\n",
        "# Sauvegarder sur Google Drive si disponible\n",
        "drive_path = \"/content/drive/MyDrive/PaniniFS_Processing\"\n",
        "if os.path.exists(drive_path):\n",
        "    try:\n",
        "        # Copier tous les fichiers\n",
        "        shutil.copy2(zip_filename, drive_path)\n",
        "        shutil.copy2(report_filename, drive_path)\n",
        "        if file_metadata:\n",
        "            shutil.copy2(csv_filename, drive_path)\n",
        "        if os.path.exists('paniniFS_real_data_analysis.png'):\n",
        "            shutil.copy2('paniniFS_real_data_analysis.png', drive_path)\n",
        "        \n",
        "        print(f\"☁️ Résultats sauvegardés sur Google Drive: {drive_path}\")\n",
        "        print(f\"   📁 Accessible depuis votre Drive: PaniniFS_Processing/\")\n",
        "        print(f\"   💾 {len(file_metadata) if file_metadata else 0} de vos fichiers analysés disponibles!\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erreur sauvegarde Drive: {e}\")\n",
        "\n",
        "# Téléchargement automatique\n",
        "print(f\"\\n⬇️ TÉLÉCHARGEMENT AUTOMATIQUE...\")\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(f\"✅ Package téléchargé: {zip_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Erreur téléchargement: {e}\")\n",
        "    print(f\"📁 Fichiers disponibles localement:\")\n",
        "    print(f\"   - {zip_filename}\")\n",
        "    print(f\"   - {report_filename}\")\n",
        "\n",
        "# Résumé final\n",
        "print(f\"\\n🎉 ANALYSE COMPLÈTE DE VOS DONNÉES TERMINÉE!\")\n",
        "print(f\"📊 {len(file_metadata) if file_metadata else 0} de vos fichiers réels analysés\")\n",
        "print(f\"🔬 {len(documents):,} documents total traités\")\n",
        "print(f\"⚡ Performance: {len(documents)/total_time:.0f} docs/sec avec GPU\")\n",
        "print(f\"🎯 Qualité: {silhouette_score:.3f} silhouette score\")\n",
        "print(f\"\\n🚀 Prêt pour intégration dans PaniniFS production!\")\n",
        "print(f\"💡 Vos patterns sémantiques sont maintenant cartographiés!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
